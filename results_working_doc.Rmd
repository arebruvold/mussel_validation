---
title: "Au validation"
output:
  bookdown::word_document2:
    latex_engine: xelatex
    theme: journal
    code_folding: hide
    number_sections: false
editor_options:
  markdown:
    wrap: 150
always_allow_html: yes
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  echo = TRUE,
  warning = FALSE
)

library(targets)
library(multcomp)
library(tidyverse)
library(kableExtra)
library(broom)
library(patchwork)
library(see)
library(metRology)
library(scales)

# library(flextable)
library(gt)
source("~/Documents/GitHub/sp_development/r/sp_funs_equal_h_thr.R")
theme_set(
  theme_linedraw(base_size = 14) +
  theme(
    panel.grid.major = element_line(size = 0.05),
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text.x = element_text(face = "bold")
  ) +
  theme(panel.grid.minor = element_line(size = 0.05))
)
```

## Resubmission JAGF

### Reviewer 1

Refer to Katrin email to see questions, Reviewer 1.

0.  Higlight novel points vs existing studies: Precision, enzyme, algorithm: ARE
1.  Rewrite highlights - same as abstract. (check if we really need highlights in this JAGF? ARE
2.  Aggregation problem, not addressed: ARE
    -   Discuss how stability may be changed
    -   Appear not to agglomerate: Show that sample prep does not induce aggl.
    -   Cannot exclude the possibility that natural particles behave diff and aggr, or are already agglomerated.
3.  Why protamex: Andre + are send file
    -   Prize: Calculate cost per sample
    -   Make sure Robustness, Simplcity, Non-hazardous is highlighted and:
    -   Argue for novelty (in introd, vs other papers use either ..)
4.  Specificity ti vs tio2: 
    -   Mention, Ti-based particles. Mass equivalent diameter for Ti.
    -   Check that density data is present in MS and that assumption is clear.
5.  Ceramic tools Cr
    -   Kitchen machine, mention.
    -   See if data on cont of Cr is present in poster data.
    -   Just show the data, qualitative. Or quantitative. "To exclude the possibility of Cr contamination ..." ADD DATA.
    -   Maybe Andre has some data in microscope?
6.  Methods:
    -   How introduced: amount, time
    -   Clarify in methods that the method applicability demonstration is for Ti, Cu, Cr.
7.  Reagent blank
    -   Change name in MS and tables
    -   Check defeinition of method blank. Dont change name for now.
8.  Selectivity not adressed esp for Cr.
    -   Mention interferences for Cr, comment if OK. Polyatomic only background, not relevant for particles
    -   Check if we have background. If not background, highly unlikely we have "polyatomic" particles
9.  particle diameter, particle mass concentration and particle number concentration of unspiked mussel were not "non-detected". Can such mussel
    really contain so many gold NPs? Please clarify.
    -   Remove low samples, exchange w \<LOD in table 1 and text if relevant.
10. Ignore. Or reinforce that dilution is nice for SP-ICP-MS except if you have contamination issues.
11. Dl Au 1.7 ng/g. Recovery range 40 - 120 at this level?
    -   Check that we compared recoveries with other studies.
    -   Include any regulatory limits?
    -   Add statment that no criteria for NPs, as such we use codex for relevant levels.
12. How about validation for Cr, Cu, Ti?
    -   Ignore?
13. Surveillance sample better homogeneity?
    -   Explain that this is due to diff texture of mussels
    -   More heterogenous sample?
14. Add the digit so that not 0.
15. New paper: An Approach Based on an Increased Bandpass for Enabling the Use of Internal Standards in Single Particle ICP-MS: Application to AuNPs
    Characterization."Please cite"

-   Add IS to avoid high dilution factor?
-   Not relevant?

### Reviewer 2

1.  Change wording to "avoid" or similar.
2.  Please describe the significance of the selection of blue mussels and the innovation point of this work, we just change matrix
    -   Composition very important
    -   Blue mussels important because..
    -   Strengthen highlights as mentioned w rev 1, n0.
    -   
3.  Look for data on spiking Au to mussels diluted 500x or check that no increase in ionic background and add sentence about this. And check if higher
    dilution mussels have been spiked with gold.
      - Are and Andre
4.  It is well-established that using "expert judgement" is common practice. Reference GUM (Stig).
    - Stig reference GUM?

### Reviewer 3

1.  Reference code in github and specify further for "threshold was calculated based on the assumption that the background noise followed a Poisson
    distribution, with a constraint of a 95% probability of observing no more than one false positive per minute". How was this determined?
    Poisson-normal or exact?
    - Are

2.  Explain high number of particles in blanks for Au. Carryover.
    - Andre

3.  Artifact: explain that not due to split events or processing of dissolved background. It happens when NO background, if background not relevant
    anymore.
      - Are

4.  Add references on matrix matching as a way to overcome, including own paper (:D)
    - Refer to own paper, Are.

5.  Matrix effect on Ti, Cu, Cr at 5k dilution. Have any data? Mention that 1/10th of matrix.
    - Are/ANDRE

6.  "The authors need to prove that the Cu NPs are not really just TiO signals recorded from large Ti-containing particles".
    -   Andre/ARE refer to oxide formation percentages max in Agilent 7900.
    -   Are/Andre find data with spiked TiO2 and check that no Cu particles.
    -   How large copper particle can a Ti-particle be detected as given interference.

## Introduction

## Data processing

## Data cleaning

```{r cleaning}
au_alldays_cleaned <- tar_read(au_alldays) %>%
  filter(
    !str_detect(sample_name, "Au\\+")
  ) %>%
  mutate(
    replicate = if_else(
      str_detect(sample_name, "^\\dB") | str_detect(sample_name, "^\\sB") | str_detect(sample_name, "Ringtest B"),
      2, 1
    ),
    sample_group = sample_name,
    sample_group = case_when(
      str_detect(sample_name, "matriseblank") ~ "Matrix blank",
      str_detect(sample_name, "blåskjell") ~ "Blue mussel",
      str_detect(sample_name, "metodeblank") ~ "Method blank",
      str_detect(sample_name, "metodeblank") ~ "Method blank",
      str_detect(sample_name, "Ringtest") ~ "Ringtest",
      str_detect(sample_name, "RM") ~ "Instrument blank",
      str_detect(sample_name, "vann") ~ "Instrument blank",
      TRUE ~ sample_group
    ),
    spike = case_when(
      str_detect(sample_name, "30\\s\\+\\s60") ~ "30 and 60 nm",
      str_detect(sample_name, "30") ~ "30 nm",
      str_detect(sample_name, "60|Ringtest") ~ "60 nm",
      TRUE ~ ""
    )
  ) %>%
  select(day, sample_group, spike, replicate, mean_size, mass_conc, particle_conc, n_particles, transport_efficiency, mass_thr, mass_signal, peaks) %>%
  # add replicate # to water samples aka instrument blank samples using join.
  group_by(day, sample_group, spike) %>%
  mutate(
    replicate = if_else(str_detect(sample_group, "Instrument blank"), cumsum(replicate), replicate)
  ) %>%
  # remove André's imported duplicates
  distinct(day, sample_group, spike, replicate, .keep_all = TRUE) %>%
  # remove additional water sample in last day
  filter(!(replicate == 7 & sample_group == "Instrument blank")) %>%
  ungroup()

write_excel_csv(au_alldays_cleaned %>% select(-peaks), file = paste0(Sys.Date(), "_au_alldays_cleaned.csv"))

```

## Au overview

```{r sample-tables}
# day on x-axis, sample on y. One table for each parameter.
# units is ng/kg mussel tissue, or particles per kg mussel tissue.
validation_parameters <- list("particle_conc", "mass_conc", "mean_size")

gt_tables_list <- validation_parameters %>%
  map(
    .f = ~ {
      validation_parameter <- .x

      gt_table <- au_alldays_cleaned %>%
        # change to ng/kg mussel tissue from ng/L in measured aliquot
        mutate(mass_conc = mass_conc/2,
               particle_conc = particle_conc/2) %>% 
        filter(sample_group %in% c("Blue mussel", "Method blank", "Instrument blank", "Matrix blank", "Deionized water")) %>%
        filter(str_detect(spike, "nm")) %>% 
        select(day, sample_group, spike, replicate, any_of(validation_parameter)) %>%
        pivot_wider(names_from = day, values_from = validation_parameter) %>%
        unite(
          "Sample", sample_group, spike,
          sep = " + "
        ) %>%
        arrange(desc(Sample)) %>%
        mutate(Sample = str_remove_all(Sample, "[\\+\\s]{1,2}$")) %>%
        # rename_with("Blue mussel", !matches("nm")) %>%
        gt(
          rowname_col = "replicate",
          groupname_col = "Sample"
        )

      {
        if (validation_parameter == "particle_conc") {
          gt_table %>%
            tab_header(title = "Particle number concentration") %>%
            tab_spanner(
              label = "Day",
              columns = c(3, 4, 5, 6, 7)
            ) %>%
            tab_stubhead(label = "Sample/ Replicate") %>%
            fmt_scientific(
              columns = 3:7,
              decimals = 2
            )
        } else if (validation_parameter == "mass_conc") {
          gt_table %>%
            tab_header(title = "Particle mass concentration") %>%
            tab_spanner(
              label = "Day",
              columns = c(3, 4, 5, 6, 7)
            ) %>%
            tab_stubhead(label = "Sample/ Replicate") %>%
            fmt_number(
              columns = 3:7,
              decimals = 2
            )
        } else if (validation_parameter == "mean_size") {
          gt_table %>%
            tab_header(title = "Mean particle diameter") %>%
            tab_spanner(
              label = "Day",
              columns = c(3, 4, 5, 6, 7)
            ) %>%
            tab_stubhead(label = "Sample/ Replicate") %>%
            fmt_number(
              columns = 3:7,
              decimals = 2
            )
        } else {
          print("Error in gt_table formatting function.")
        }
      }
    }
  )

gt_tables_list[[1]]

gt_tables_list[[2]]

gt_tables_list[[3]]

```

```{r}
gt_tables_list[[1]] %>% gtsave(filename = "pnc_table.docx")

gt_tables_list[[2]] %>% gtsave(filename = "mc_table.docx")

gt_tables_list[[3]] %>% gtsave(filename = "ms_table.docx")
```

## Precision

```{r ANOVA-precision}

# fit lm for each spike level, parameter (pnc/mass/size) ~ day.
# comment: Warning in code is due to sample without replicates.

precision_anovas <-
  au_alldays_cleaned %>%
  mutate(
    day = as.factor(day),
    replicate = as.factor(replicate)
  ) %>%
  filter(sample_group == "Blue mussel") %>%
  select(-n_particles, -transport_efficiency) %>%
  pivot_longer(
    cols = c(mean_size, mass_conc, particle_conc),
    names_to = "parameter",
    values_to = "value"
  ) %>%
  group_by(spike, parameter) %>%
  nest() %>%
  ungroup() %>%
  mutate(
    anovas = map(
      data,
      ~ .x %>%
        lm(formula = value ~ day) %>%
        anova() %>%
        tidy() %>%
        select(term, sumsq, meansq) %>%
        pivot_wider(names_from = term, values_from = c(meansq, sumsq))
    ),
    grand_means = map(data, ~ .x %>%
      ungroup() %>%
      summarise(grand_mean = mean(value)))
  ) %>%
  unnest(anovas, grand_means) %>%
  # Precision parameters
  mutate(
    RSD_repeatability = sqrt(meansq_Residuals) / grand_mean,
    RSD_intermediate_precision = sqrt(
      sqrt(meansq_Residuals)^2 +
        sqrt(
          (meansq_day - meansq_Residuals) / 2
          # NOTE THAT THIS DOES NOT GENERALISE, ONLY VALID IF TWO REPLICATES
        )^2
    ) / grand_mean
  )


precision_gtable <- precision_anovas %>%
  filter(str_detect(spike, "\\d")) %>%
  select(spike, parameter, RSD_repeatability, RSD_intermediate_precision) %>%
  rename(c(sample = spike)) %>%
  mutate(parameter = case_when(
    parameter == "mass_conc" ~ "Particle mass concentration",
    parameter == "particle_conc" ~ "Particle number concentration",
    parameter == "mean_size" ~ "Mean size"
  )) %>%
  mutate(sample = paste("Blue mussel +", sample)) %>%
  gt(
    rowname_col = "sample",
    groupname_col = "parameter"
  ) #%>%
  # fmt_percent(columns = -c(1, 2)) %>%
  # cols_label(
  #   RSE_repeatability = html("RSD<sub>Repeatability</sub>"),
  #   RSE_intermediate_precision = html("RSD<sub>Intermediate precision</sub>")
  # )

precision_gtable

precision_gtable %>% gtsave(filename = "precision_table.docx")
  
```

## Trueness

\### As recovery

```{r trueness-vs-measured, include = FALSE}
trueness_meas_gt <- au_alldays_cleaned %>%
  filter(
    sample_group %in%
      c(
        "Blue mussel",
        "Matrix blank",
        "Method blank",
        "Instrument blank"
      ),
    spike %in% c("30 nm", "60 nm")
  ) %>%
  group_by(sample_group, spike) %>%
  summarise(across(c("particle_conc", "mass_conc", "mean_size"), ~ mean(.))) %>%
  group_by(spike) %>%
  mutate(across(c("particle_conc", "mass_conc", "mean_size"), ~ .x / .x[sample_group == "Instrument blank"])) %>%
  select(sample_group, spike, mean_size, mass_conc, particle_conc) %>%
  gt() %>%
  fmt_percent(
    columns = !matches("sample_group"),
    decimals = 0
  ) %>%
  cols_label(
    mean_size = "Particle diameter",
    mass_conc = "Particle mass concentration ",
    particle_conc = "Particle number concentration",
    sample_group = "Sample"
  )

trueness_meas_gt %>%
  gtsave(filename = "trueness_meas_rm_gt.docx")

trueness_meas_gt
  
```

```{r trueness-vs-theoretical}
# NPC: 4.58*10^7 or 3.66*10^8.
# Str: 30 og 60.
# Mass conc: 100 ng/L.

trueness_theo_gt <- au_alldays_cleaned %>%
  filter(
    sample_group %in%
      c(
        "Blue mussel",
        "BM spike after digest",
        "Method blank",
        "RM"
      ),
    spike %in% c("30 nm", "60 nm")
  ) %>%
  group_by(sample_group, spike) %>%
  summarise(across(c("particle_conc", "mass_conc", "mean_size"), ~ mean(.))) %>%
  group_by(spike) %>%
  mutate(
    particle_conc = if_else(spike == "30 nm", particle_conc / (3.66*10^8) , particle_conc / (4.58*10^7)),
    mass_conc = mass_conc/100,
    mean_size = if_else(spike == "30 nm", mean_size / 30 , mean_size / 60)
  ) %>% 
  select(sample_group, spike, mean_size, mass_conc, particle_conc) %>% 
  gt() %>% 
    fmt_percent(
    columns = !matches("sample_group"),
    decimals = 0
  ) %>% 
     cols_label(
    mean_size = "Particle diameter",
    mass_conc = "Particle mass concentration ",
    particle_conc = "Particle number concentration"
  )

trueness_theo_gt

trueness_theo_gt %>%  gtsave(filename = "trueness_theo_gt.docx")
  
```

### From proficiency test

```{r}
au_alldays_cleaned %>% filter(sample_group == "Ringtest") %>%
  summarise(across(c("particle_conc", "mean_size"), ~ mean(.))) %>%
  mutate(
    particle_conc_recovery = particle_conc/(1.44*10^8),
    mean_size_recovery = mean_size/61
    
  ) %>% 
  mutate(zscore_pnc = (particle_conc - (1.44*10^8)) / (0.65*10^8),
         zscore_size = (mean_size - 61) / 6.1 ) %>% gt() %>% fmt_number(columns = -particle_conc,decimals = 2)

```

## Selectivity

-   Is there interferences on the measurement of the analyte of interest?
    -   Presence in blank.
    -   Change in spiked signal.

### Table

```{r}

au_alldays_cleaned %>%
  mutate(
    mass_conc = mass_conc / 2,
    particle_conc = particle_conc / 2
  ) %>%
  filter(
    str_detect(sample_group, "blank"),
    str_detect(sample_group, "Method", negate = TRUE)
  ) %>%
  group_by(
    sample_group, spike
  ) %>%
  summarise(
    across(
      .cols = c(mean_size, mass_conc, particle_conc),
      list(
        mean = ~ mean(.x, na.rm = TRUE),
        sd = ~ sd(.x, na.rm = TRUE)
      )
    )
  ) %>% 
  gt(rowname_col = "spike") %>%
  fmt_number(
    columns = 3:6,
    decimals = 1
  ) %>%
  fmt_scientific(
    7:8,
    rows = everything(),
    decimals = 2,
    drop_trailing_zeros = FALSE,
    scale_by = 1,
    pattern = "{x}",
    sep_mark = ",",
    dec_mark = ".",
    # force_sign = FALSE,
    locale = NULL
  ) %>%
  cols_merge_uncert(mean_size_mean, mean_size_sd, sep = " ± ", autohide = TRUE) %>% 
  cols_merge_uncert(mass_conc_mean, mass_conc_sd, sep = " ± ", autohide = TRUE) %>% 
  cols_merge_uncert(particle_conc_mean, particle_conc_sd, sep = " ± ", autohide = TRUE) %>% 
   cols_label(
    mean_size_mean = "Particle diameter",
    mass_conc_mean = "Particle mass concentration",
    particle_conc_mean = "Particle number concentration"
  ) %>% 

  gtsave(filename = "selectivity_table.docx")
  
```

```{r}
# plot particle size distribution of 30nm and 60 nm side by side, superimposing in instrument blank and in matrix blank.

plot_selectivity <- au_alldays_cleaned %>% 
  filter(
    str_detect(sample_group, "blank"),
    str_detect(sample_group, "Method", negate = TRUE),
    spike != ""
  ) %>% 
  ungroup() %>%
  mutate(sample_group = case_when(
    sample_group == "Instrument blank" ~ "Spiked UPW",
    sample_group == "Matrix blank" ~ "Spiked mussel tissue",
    TRUE ~ NA
  )) %>% 
  unnest(peaks) %>% group_by(sample_group, spike) %>% 
  mutate(particle_size_mean = mean(particle_size, na.rm = TRUE)) %>% 
  ggplot(aes(particle_size,
    fill = sample_group
  )) + geom_vline(aes(xintercept = particle_size_mean), color = "red", linetype = "longdash")+
  geom_histogram(position = "identity", binwidth = 0.5, alpha = 0.6) +
  facet_wrap(~spike, scale = "free") +
  
  xlim(c(0, 75))+
  theme_linedraw(base_size = 15)+
     theme(panel.grid.major = element_line(size = 0.05))+
  theme(panel.grid.minor = element_line(size = 0.05))+
  scale_fill_manual(values=c("#436EEE", "#8B3626"))+
  xlab("Particle diameter [nm]")+
  ylab("Frequency [#]")


plot_selectivity

# showing artifact peak max distribution. These could be filtered by changing detection threshold.
# au_alldays_cleaned %>%
#   filter(
#     str_detect(sample_group, "blank"),
#     str_detect(sample_group, "Method", negate = TRUE),
#     spike != ""
#   ) %>%
#   ungroup() %>%
#   unnest(peaks) %>%
#   filter(spike == "60 nm",
#          particle_size < 25) %>% ggplot(aes(peak_max/peak_width))+geom_histogram(binwidth =0.5)+theme_bw()

ggsave(plot_selectivity, filename = "plot_selectivity.tiff", width = 15, height = 5 )
```

### Statistics

```{r}
# Instrument blank vs matrix blank (interfering species present)
# Sub-ng concentration, higher conc in mussel (avg 25 vs 15).
# Statistical significance driven by outlier in matrix blank.

list("mass_conc", "n_particles") %>%
  map(
    ~ {
        t.test(
          au_alldays_cleaned %>%
            filter(
              sample_group == "Instrument blank",
              spike == ""
            ) %>% pull(.x),
          au_alldays_cleaned %>%
            filter(
              sample_group == "Matrix blank",
              spike == "",
              day != 1
            ) %>% pull(.x)
        )
    }
  )

# Instrument blank spiked vs matrix blank spiked (matrix effects)

# mass_conc, particle conc, mean size
# 30 nm
list("mass_conc", "particle_conc", "mean_size") %>%
  map(
    ~ {
        wilcox.test(
          au_alldays_cleaned %>%
            filter(
              sample_group == "Instrument blank",
              spike == "30 nm"
            ) %>% pull(.x),
          au_alldays_cleaned %>%
            filter(
              sample_group == "Matrix blank",
              spike == "30 nm"
            ) %>% pull(.x)
        )
    }
  )


# 60 nm
list("mass_conc", "particle_conc", "mean_size") %>%
  map(
    ~ {
        t.test(
          au_alldays_cleaned %>%
            filter(
              sample_group == "Instrument blank",
              spike == "60 nm"
            ) %>% pull(.x),
          au_alldays_cleaned %>%
            filter(
              sample_group == "Matrix blank",
              spike == "60 nm"
            ) %>% pull(.x)
        )
    }
  )

# sensitivity via particle mass

# 30 nm

        t.test(
          au_alldays_cleaned %>% unnest(peaks) %>% 
            filter(
              sample_group == "Instrument blank",
              spike == "30 nm"
            ) %>% pull(particle_mass),
          au_alldays_cleaned %>% unnest(peaks) %>% 
            filter(
              sample_group == "Matrix blank",
              spike == "30 nm"
            ) %>% pull(particle_mass)
        )
1-3.910639e-19/4.829401e-19

# 60 nm

        t.test(
          au_alldays_cleaned %>% unnest(peaks) %>% 
            filter(
              sample_group == "Instrument blank",
              spike == "60 nm"
            ) %>% pull(particle_mass),
          au_alldays_cleaned %>% unnest(peaks) %>% 
            filter(
              sample_group == "Matrix blank",
              spike == "60 nm"
            ) %>% pull(particle_mass)
        )
1-1.788208e-18/2.274148e-18

```

We have a marginal significant difference between unspiked matrix blank and instrument blank in terms of particle mass or particle number
concentrations. Yet, this in both cases sub-ng/L and mostly driven by single outlier. Hence, no interfering species appear to be present.

We have a significant difference between matrix blank instrument blank in terms of particle mass concentration and mean particle size, but not
particle number concentration. This indicates that we do not have a change in transport efficiency, yet in the signal per particle.

```{r}
# #pragmatic/preliminary for presentation
# tar_read(au_alldays) %>% filter(str_detect(sample_name, "60 nm Au 100 ng/l RM|2B blåskjell 60 nm Au 100 ng/l")) %>% 
# rowwise() %>% unnest(peaks) %>% group_by(sample_name, day) %>% 
#   summarise(median_peak_area = median(peak_area, na.rm = TRUE),
#             mean_particle_size = mean(particle_size, na.rm = TRUE),
#             mean_particle_conc = mean(particle_conc, na.rm = TRUE),
#             mean_mass_conc = mean(mass_conc, na.rm = TRUE)) %>% 
#   group_by(sample_name) %>% 
#   summarise(
#     mean_peak_area = mean(median_peak_area, na.rm = TRUE),
#             mean_particle_size = mean(mean_particle_size, na.rm = TRUE),
#             mean_particle_conc = mean(mean_particle_conc, na.rm = TRUE),
#             mean_mass_conc = mean(mean_mass_conc, na.rm = TRUE)
#   )
# 
# # Size matrix fx: `r 53.4/59.10` signal matrix fx`r 348.8/463.2` pnc sign `r 20567565/20366243` mc `r 35.67223/46.32001`
```

## Working range

```{r}

# Make calibration curves for each day-batch, appending day number.
calibration <- list.files("~/sp-data/paper0_au_validering", full.names = TRUE) %>%
  map_df(.f = ~ sp_classifier(.x, "60 nm Au 100 ng/l RM") %>%
    sp_response() %>%
    mutate(day = str_extract(.x, "\\d(?=.b$)"))) %>%
  distinct(response, .keep_all = TRUE) %>%
  select(day, intercept, response, r2)

calibration %>%
  rename_with(~ .x %>%
    str_to_title()) %>%
  gt() %>%
  fmt_number(
    columns = 2:3,
    decimals = 2
  ) %>%
  fmt_number(
    columns = 4,
    decimals = 5
  ) %>%
  gtsave(filename = "calibration_curves.docx")

```

### Particle number concentration

Assuming the particles are homogeneously dispersed, the number of particles measured in a given interval will follow a Poisson distribution. The rate
parameter, $\lambda$, corresponds to the mean number of particles in this interval. It follows from number theory that the interarrival times of such
events will follow an exponential distribution with rate parameter $\lambda$.

We have that $\lambda = c_n \cdot \frac{Q_{det}}{t_{meas}}$, where $c_n$ is the particle number concentration of the measured solution in particles/L,
$Q_{det}$ is the volumetric mass transport through the ICP in L per total acquisition time and $t_{meas}$ is the number of dwells per acquisition
time.

Using the cumulative exponential distribution we obtain

$$F(w_p)=\left\{\begin{aligned} 0, & \text { if } w_p<0 \\ 
1-e^{\frac{-c_n \cdot Q_{det} \cdot w_p}{t_{meas}}}, 
& \text { if } w_p \geq 0\end{aligned}\right.$$

Thus, accepting 5% particle coincidence or overlap, we have:

$$\frac{t_{meas} \cdot \ln{(0.95^{-1})}}{w_p \cdot Q_{det}} = c_n.$$

Inserting the flow rate through the plasma and peak widths obtained for 60 nm:

$c_n = \frac{\ln \left(\frac{1}{0.95}\right)}{\left(12.63 \times 3.407 \times 10^{-11}\right)}$ = $1.2{\cdot 10^8}$ particles per liter.

And similarly for 30 nm:

$c_n = \frac{\ln \left(\frac{1}{0.95}\right)}{\left(9.670 \times 3.407 \times 10^{-11}\right)}$ = $1.55{\cdot 10^8}$ particles per liter.

Taking dilution factor into account, we get for 60 nm `r 500*log(1/0.95)/(12.62884*3.407108*10^-11)` particles per kg mussel tissue and 30 nm
`r 500*log(1/0.95)/(9.699962*3.407108*10^-11)` particles per kg mussel tissue.

## Detection limit

### Critical size

A peak width of 500 $\mu$s is assumed based on approximation of peak widths near the LOD and literature (Laborda 2020). Assuming a triangular peak
shape, it is possible to obtain an estimate for the corresponding particle mass, denoted as $m_p$:

$$m_{p} = \frac{ A_p}{ S } = \frac{w_p \cdot h_p}{2S},$$

where $A_p$ is the particle signal, S is the sensitivity in counts per kg element. Using peak width 5 and height 6, we get $$5*6*1.698164e-21/2$$
gives 2.547246e-20 for the peak mass at LOD.

using $$d = (\frac{6m}{\pi p})^{1/3}$$ we get

```{r peak-width}
peak_width <- 5

#size/mass detection limit
dl_sizemass <- au_alldays_cleaned %>%
  filter(sample_group == "Matrix blank") %>%
  filter(spike == "") %>%
  unnest(peaks) %>%
  group_by(day) %>%
  summarise(h_thr = median(h_thr, na.rm = TRUE),
            mass_thr = median(mass_thr, na.rm = TRUE)) %>%
  mutate(
    mass_detection_limit = peak_width * mass_thr/2,
    size_detection_limit = ((6*mass_detection_limit)/(pi*19.32*1000))^(1/3)
  )
    
```

### Concentrations

```{r}

#particle conc detection limit, using pooled blanks and mean number of particles detected. Converted to pnc by using the mean volume flow. Including dilution factor.
  
 dl_pnc <- au_alldays_cleaned %>%
   filter(str_detect(sample_group, "blank")) %>%
   filter(spike == "") %>% # group_by(day) %>%
   summarise(
     pnc_dl_pois =  poisson.test(mean(n_particles) %>% round(digits = 0), conf.level = 0.997)$conf.int[2] * mean(particle_conc) / (2*mean(n_particles)),
     pnc_dl_gaussian =  (mean(n_particles) + 3 * sd(n_particles)) * mean(particle_conc) / (2*mean(n_particles)),
     mass_conc_dl_gaussian =  (mean(mass_conc) + 3 * sd(mass_conc))/2
   )
 
```

## Uncertainty

To determine the signal per element mass, $S$, a particulate reference material with a known mass or size can be analyzed. By integrating
background-subtracted peaks and using the median or the kernel density estimate of the resulting signal distribution, we obtain an estimate of the
signal per particle mass or size in [counts/kg]:

$$S =  \frac{6 A_{rm}}{\rho  \cdot \pi \cdot d_{rm}^3},$$

where $\rho$ is the element density, $d_{rm}$ the particle diameter and $A_{rm}$ is the estimated signal per particle in counts.

Using ionic standards to establish a calibration curve, we use the slope coefficient, $b$, and the concentration of the corresponding ionic standard,
$C_{std}$, for an estimate for the signal per dwell per mass element per volume in $\frac{counts}{dwell}/ \frac{kg}{L}$:

$$I = \frac{b}{C_{std}}.$$

Combining these and multiplying by $t_{meas}$, the number of dwells during acquisition, the flow rate through the plasma, $Q_{det}$, in
$\frac{L}{t_{acq}}$ may be determined and used to relate measured quantities to concentrations:

$$Q_{det} = \frac{t_{meas} \cdot I}{S}. $$

Mean particle diameter is then given by

$$ \overline d_{p} =  \frac{1}{n}\sum_{i=1}^nd_{p_i} =
\frac{1}{n}\sum_{i=1}^n\sqrt[3]{\frac{6  S \cdot A_{p_i}}{\pi \rho} } = 
\frac{d_{rm}}{n\cdot \sqrt[3]{A_{rm}}}\sum_{i=1}^n\sqrt[3]{A_{p_i} },$$

the particle mass concentration, $c_m$, by

$$c_m = \frac{\sum_{i=1}^nA_{p_i} \cdot S_{}}{Q_{det}} = \frac{ C_{std} \sum_{i=1}^nA_{p_i}}{t_{meas} \cdot b}, $$

and the particle number concentration, $c_n$, by

$$c_{n} = \frac{n_{meas}}{Q_{det}}.$$

Inserting for $Q_{det}$, we get

$$ c_{n}  = \frac{n_{meas} \cdot 6 A_{rm} \cdot C_{std}}{t_{meas} \cdot \rho  \cdot \pi \cdot d_{rm}^3 \cdot b} $$

-   $n_{meas}$ is the instrument measured number of particle events. This corresponds to a Poisson process and hence the standard uncertainty will be
    its square root. Using the LOD for particle number concentration based on 35 measured particles justifies a normal approximation as a conservative
    estimate for the uncertainty.

-   $A_{rm}$ in counts is the location of the peak area per particle event of the reference material as determined using a kernel density estimate,
    uncertainty set to 5%.

-   $A_p$ in counts is the peak area or total signal of a particle event of which 5 % uncertainty is assumed.

-   $C_{std}$ [kg/L] is the concentration of the standard. We use the uncertainty of the standard at 1%, and add another multiplicative term with 5%
    to account for the uncertainty of preparation.

-   $t_{meas}$ is the number of dwells per measurement, uncertainty set to 0.5%.

-   $\rho$ [kg/m\^3] is the element density of a particle. Its uncertainty is not well-established, yet [@montoro_bustos_evaluation_2022] 0.05%. We
    set 1%.

-   $d$ [m] is the particle diameter. For the reference material, the uncertainty in the certificate of analysis at 4 % is used.

-   $b$ [counts/dwell] is the slope coefficient of the calibration curve. We use the uncertainty of the the linear regression on each day.

For each variable the standard uncertainty is assumed for a Monte Carlo simulation with $10^6$ repeats, reporting the uncertainty with a coverage
factor of two.

```{r standard-uncert-pnc}

set.seed(123)
uncertainties <- c(1, 2, 3, 4, 5) %>%
  map_dfr(
    ~ tar_read(au_alldays) %>%
      filter(str_detect(sample_name, "Au\\+|60.*RM"), day == .x) %>%
      {
        d_pip <- 1
        d_pip_su <- 0.05
        n_meas <- 1000 # n particles
        n_meas_su <- sqrt(n_meas)
        t_meas <- 1200000 # n dwells
        t_meas_su <- 1200000 * 0.005
        d_rm <- 60 * 10^-9 # particle diameter reference material
        d_rm_su <- d_rm * 0.04 # standard uncertainty particle diameter reference material
        rho <- 1000 * 19.32 # density Au SI kg/m3
        rho_su <- rho * 0.01 # uncertainty density
        A_rm <- (.) %>%
          filter(sample_name == "60 nm Au 100 ng/l RM") %>%
          unnest(peaks) %>%
          pull(peak_area) %>%
          {
            dens <- density((.), na.rm = TRUE)
            dens$x[which.max(dens$y)]
          } # KDE
        A_rm_su <- A_rm * 0.05
        A_p <- (.) %>%
          filter(sample_name == "60 nm Au 100 ng/l RM") %>%
          unnest(peaks) %>%
          pull(peak_area) %>%
          mean(na.rm = TRUE)
        A_p_su <- A_p * 0.05
        A_psum <- (.) %>%
          filter(sample_name == "60 nm Au 100 ng/l RM") %>%
          unnest(peaks) %>%
          pull(peak_area) %>%
          sum(na.rm = TRUE)
        A_psum_su <- A_psum * 0.05
        pi <- pi
        pi_su <- 0
        R_std <- (.) %>% # response, counts per kg/L
          filter(str_detect(sample_name, "Au\\+")) %>%
          ungroup() %>%
          mutate(
            conc_ppb = str_extract(sample_name, str_extract(
              sample_name,
              paste0("[\\d.]+(?=[\\sa-zA-Z]*", isotope, ")")
            )) %>% as.numeric()
          ) %>%
          mutate(
            intercept =
              coef(
                lm(formula = as.numeric(mean_counts) ~ conc_ppb)
              )[1],
            response =
              coef(
                lm(formula = as.numeric(mean_counts) ~ conc_ppb)
              )[2],
            r2 = summary(
              lm(formula = as.numeric(mean_counts) ~ conc_ppb)
            )$r.squared
          ) %>%
          ungroup() %>%
          pull(response) %>%
          median(na.rm = TRUE)

        R_std_su <- (.) %>% # response, counts per kg/L
          filter(str_detect(sample_name, "Au\\+")) %>%
          ungroup() %>%
          mutate(
            conc_ppb = str_extract(sample_name, str_extract(
              sample_name,
              paste0("[\\d.]+(?=[\\sa-zA-Z]*", isotope, ")")
            )) %>% as.numeric()
          ) %>%
          lm(formula = as.numeric(mean_counts) ~ conc_ppb) %>%
          summary()
        R_std_su <- R_std_su$coefficients[, "Std. Error"][2] %>% as.numeric()

        C_std <- 1 * 10^(-9) # 1 ug/L = 10^-9 kg/L
        C_std_su <- C_std * 0.01
        
    # expression mean diameter
        expr_md <- expression(d_rm * (A_p / A_rm)^(1 / 3))
        # values for mean diameter variables
        x_md <- list(d_rm = d_rm, A_p = A_p, A_rm = A_rm)
        # uncertainties for mean diameter vars
        u_md <- list(d_rm = d_rm_su, A_p = 3*A_p_su/sqrt(n_meas), A_rm = A_rm_su)
        # uncertainties for mean diameter
        u.expr_md <- uncert(expr_md, x_md, u_md, method = "MC", B = 1000000)
        u.expr_md$u.y

        # expression particle number concentration
        expr_pnc <- expression(d_pip * n_meas * (A_p * C_std) / (t_meas * rho * (4 / 3) * pi * (d_rm / 2)^3 * R_std))

        # values for each variable for particle number concentration
        x_pnc <- list(d_pip = d_pip, t_meas = t_meas, n_meas = n_meas, A_p = A_p, C_std = C_std, rho = rho, pi = pi, d_rm = d_rm, R_std = R_std)
        
        # uncertainties for particle number concentration variables
        u_pnc <- list(d_pip = d_pip_su, t_meas = t_meas_su, n_meas = n_meas_su, A_p = A_p_su, C_std = C_std_su, rho = rho_su, pi = pi_su, d_rm = d_rm_su, R_std = R_std_su)
        # uncertaintites for particle number concentration
        u.expr_pnc <- uncert(expr_pnc, x_pnc, u_pnc, method = "MC", B = 1000000)
        u.expr_pnc$u.y

    
        
        # expression particle mass concentration
        expr_pmc <- expression(n_meas*d_pip*C_std*A_psum/(1000*t_meas*R_std))
        # values for pmc variables
        x_pmc <- list(n_meas = n_meas, d_pip = d_pip, C_std = C_std, A_psum = A_psum, t_meas = t_meas, R_std = R_std)
        # uncertainties for pmc vars
        u_pmc <- list(n_meas = n_meas_su, d_pip = d_pip_su, C_std = C_std_su, A_psum = A_psum_su, t_meas = t_meas_su, R_std = R_std_su)
        # uncertainties for pmc
        u.expr_pmc <- uncert(expr_pmc, x_pmc, u_pmc, method = "MC", B = 1000000)
        u.expr_pmc$u.y
list(
        tibble(
          uncertainty_md = (u.expr_md$u.y / (u.expr_md$y)),
          uncertainty_pmc = (u.expr_pmc$u.y / (u.expr_pmc$y)),
          uncertainty_pnc = u.expr_pnc$u.y / u.expr_pnc$y,
          # md=u.expr_md$y,
          # pmc=u.expr_pmc$y,
          # pnc=u.expr_pnc$y,
          day = .x
        )%>% mutate(
  across(starts_with("uncertainty"), ~ .x * 2)
)
# ,
#         plot.uncert(u.expr_md),
#         plot.uncert(u.expr_pmc),
#         plot.uncert(u.expr_pnc)
)
      }
  )

uncertainties %>% 
  summarise(across(everything(), ~ mean(.x))) %>% 
  gt() %>%
  fmt_percent(
    columns = !matches("day"),
    decimals = 1
  ) %>%
  cols_label(
    uncertainty_md = "Mean particle diameter",
    uncertainty_pmc = "Particle mass concentration",
    uncertainty_pnc = "Particle number concentration"
  )
```

## Stability

Particle stability assessed by comparison of spiked blanks versus mussel samples spiked before and after digestion.

```{r}
au30_stab <- au_alldays_cleaned %>%
  filter(
    str_detect(spike, "^\\d{2}\\snm"),
    sample_group %in% c(
      "Method blank",
      "Matrix blank",
      "Blue mussel",
      "Instrument blank"
    ),
    spike == "30 nm"
  ) %>%
  mutate(sample_group = sample_group %>% as.factor()) %>%
  ungroup() %>%
  select(sample_group, mass_conc, particle_conc)

amod_mass_conc <- aov(mass_conc ~ sample_group, data = au30_stab)

amod_particle_conc <- aov(particle_conc ~ sample_group, data = au30_stab)

glht(amod_mass_conc, linfct = mcp(sample_group = "Tukey")) %>% summary()

glht(amod_particle_conc, linfct = mcp(sample_group = "Tukey")) %>% summary()

au60_stab <- au_alldays_cleaned %>%
  filter(
    str_detect(spike, "^\\d{2}\\snm"),
    sample_group %in% c(
      "Method blank",
      "Matrix blank",
      "Blue mussel",
      "Instrument blank"
    ),
    spike == "60 nm"
  ) %>%
  mutate(sample_group = sample_group %>% as.factor()) %>%
  ungroup() %>%
  select(sample_group, mass_conc, particle_conc)

amod_mass_conc <- aov(mass_conc ~ sample_group, data = au60_stab)

amod_particle_conc <- aov(particle_conc ~ sample_group, data = au60_stab)

glht(amod_mass_conc, linfct = mcp(sample_group = "Tukey")) %>% summary()

glht(amod_particle_conc, linfct = mcp(sample_group = "Tukey")) %>% summary()

```

## Poster


```{r}
# LODs, blank calcs:

lod_blk_calculator <- function(processed_batch, acq_time) {
  LODs <- processed_batch %>% select(-matches("_LOD")) %>% 
    select(-peaks) %>% 
    filter(str_detect(sample_name, "PrBlank\\s\\d")) %>%
    group_by(isotope) %>%
    summarise(
      # LODs from procedural blanks
      mass_conc_LOD =  3 * sd(mass_conc),
      total_conc_LOD =  3 * sd(total_conc),
      n_particles_LOD = PoiCI(mean(n_particles), 1 - 0.003),
      particle_conc_LOD = n_particles_LOD / (mean(detector_flow_rate)*1800000),
      # mean blank values from procedural blanks
      blk_mass_conc = mean(mass_conc),
      blk_total_conc = mean(total_conc),
      blk_n_particles = mean(n_particles),
      blk_particle_conc = mean(particle_conc))
  
  return(left_join(processed_batch, LODs, by = "isotope") %>% ungroup() )
}

# Blank subtraction from samples, addition of sample grouping.

subtracter_grouper <- function(processed_batch, acq_time) {
  processed_batch %>%
    filter(isotope != "Au" & type == "SAMPLE") %>%
    lod_blk_calculator(acq_time) %>%
    # sample grouping
    mutate(
      sample_group =
        case_when(
          str_detect(sample_name, "IBLK") ~ "IBLK",
          str_detect(sample_name, "PrBlank\\s\\d") ~ "PMBLK",
          str_detect(sample_name, "^[A-Z]{1}[\\s\\d]{1}") ~ str_extract(sample_name, "^[A-Z]{1}"),
          str_detect(sample_name, "^[A-Z]{1}S[\\s\\d]{1}") ~ str_extract(sample_name, "^[A-Z]{1}S"),
          TRUE ~ NA_character_
        ),
      parallel = str_extract(sample_name, "_p\\d{1,2}") %>% str_extract("\\d{1,2}"),
      measurement = str_extract(sample_name, "_m\\d") %>% str_extract("\\d")
    ) %>%
    # blank subtraction
    group_by(isotope) %>%
    mutate(
      particle_fraction = 100 * mass_conc / (total_conc * 1000),
      mass_conc = if_else(str_detect(sample_name, "x1000"), mass_conc - blk_mass_conc, mass_conc),
      total_conc = if_else(str_detect(sample_name, "x1000"), total_conc - blk_total_conc, total_conc),
      n_particles = if_else(str_detect(sample_name, "x1000"), as.numeric(n_particles - blk_n_particles), as.numeric(n_particles)),
      particle_conc = if_else(str_detect(sample_name, "x1000"), particle_conc - blk_particle_conc, particle_conc)
    ) %>%
    ungroup()
}
```

```{r poster-visualization}

fancy_scientific <- function(l) {
     # turn in to character string in scientific notation
     l <- format(l, scientific = TRUE)
      # fix zero
     l <- gsub("0e\\+00","0",l)
     # quote the part before the exponent to keep all the digits
     l <- gsub("^(.*)e", "'\\1'e", l)
     
          l <- gsub("e\\+","e",l)
     # turn the 'e+' into plotmath format
     l <- gsub("e", "%*%10^", l)

     
     l <- gsub("\\'1[\\.0]*\\'\\%\\*\\%", "", l)
    
     # return this as an expression
     parse(text=l)
}


# change to account for 5000 dilution factor
poster_processed <- rbind(
  tar_read(poster1) %>% subtracter_grouper(acq_time = 180) %>% mutate(day = 1),
  tar_read(poster2) %>% subtracter_grouper(acq_time = 180)%>% mutate(day = 2)
) %>% filter(!str_detect(sample_name, "^[A-Z]{1}[\\d]{1}")) %>%
  sp_comparer(acq_time = 180) %>% 
  mutate(mass_conc = 6 * mass_conc,
         particle_conc = 6 * particle_conc,
         particle_conc_LOD = 6 * particle_conc_LOD,
         mass_conc_LOD = 6 * mass_conc_LOD)

# mass concentration
poster_processed %>%
  filter(
    sample_group != "PMBLK",
    !str_detect(sample_group, "\\wS")
  ) %>%
  mutate(sample_group = case_when(
    str_detect(sample_group, "A|B|C") ~ "Farm",
    str_detect(sample_group, "G|H|I") ~ "Harbor",
    str_detect(sample_group, "J|K|L") ~ "Surveillance"
  )) %>%
  ggplot(aes(sample_group, mass_conc)) +
  stat_summary(geom = "errorbar", fun.data = mean_cl_normal) +
  geom_jitter(width = 0.2, aes(shape = as.factor(day))) +
  geom_hline(
    aes(yintercept = mean(mass_conc_LOD)),
    alpha = .25,
    linetype = "solid",
    color = "red",
    size = 2
  ) +
  facet_wrap(~isotope, scales = "free") +
  theme_linedraw(base_size = 14) +
  theme(
    panel.grid.major = element_line(size = 0.05),
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text.x = element_text(face = "bold")
  ) +
  theme(panel.grid.minor = element_line(size = 0.05)) +
  xlab("") +
  ylab("Particle mass concentration [ng/g]")+
  guides(shape=guide_legend(title="Day"))

ggsave("poster_mass_conc.png", height = 4, width = 9)

#particle conc
poster_processed %>%
  filter(
    sample_group != "PMBLK",
    !str_detect(sample_group, "\\wS")
  ) %>%
  mutate(sample_group = case_when(
    str_detect(sample_group, "A|B|C") ~ "Farm",
    str_detect(sample_group, "G|H|I") ~ "Harbor",
    str_detect(sample_group, "J|K|L") ~ "Surveillance"
  )) %>%
  ggplot(aes(sample_group, particle_conc)) +
  stat_summary(geom = "errorbar", fun.data = mean_cl_normal) +
  geom_jitter(width = 0.2, aes(shape = as.factor(day))) +
  geom_hline(
    aes(yintercept = mean(particle_conc_LOD)),
    alpha = .25,
    linetype = "solid",
    color = "red",
    size = 2
  ) +
  facet_wrap(~isotope, scales = "free") +
  theme_linedraw(base_size = 14) +
  theme(
    panel.grid.major = element_line(size = 0.05),
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text.x = element_text(face = "bold")
  ) +
  theme(panel.grid.minor = element_line(size = 0.05)) +
  scale_y_continuous(breaks = breaks_pretty(n=6, min.n = 4), labels = fancy_scientific)+
  xlab("") +
  ylab("Particle number concentration [#/g]")+
  guides(shape=guide_legend(title="Day"))

ggsave("poster_particle_conc.png", height = 4, width = 9)

```

### Poster detection limit

```{r}
peak_width <- 6

#size/mass detection limit
poster_dl_sizemass <- poster_processed %>%
  filter(sample_group == "PMBLK") %>%
  unnest(peaks) %>%
  group_by(isotope) %>%
  summarise(h_thr = median(h_thr, na.rm = TRUE),
            mass_thr = max(mass_thr, na.rm = TRUE),
            peak_width = median(peak_width, na.rm = TRUE)) %>%
  mutate(
    mass_detection_limit = peak_width * mass_thr/2,
    size_detection_limit = case_when(
      isotope == "Au" ~ ((6*mass_detection_limit)/(pi*19.32*1*1000))^(1/3),
      isotope == "Ti" ~ ((6*mass_detection_limit)/(pi*4.17*0.59880240*1000))^(1/3),
      isotope == "Cr" ~ ((6*mass_detection_limit)/(pi*5.22*0.68399453*1000))^(1/3),
      isotope == "Cu" ~ ((6*mass_detection_limit)/(pi*6.31*0.79886864*1000))^(1/3)
  ))

```

Peak width of approx 6 appears to be representative for all particle types.

### Poster precision

Per sample, per isotope.

```{r}

poster_dls <- bind_rows(
  poster_processed %>%
    group_by(isotope) %>%
    summarise(
      particle_conc = mean(particle_conc_LOD),
      mass_conc = mean(mass_conc_LOD)
    ) %>% pivot_longer(cols = c(particle_conc, mass_conc), names_to = "parameter", values_to = "detection_limit"),
  poster_dl_sizemass %>% select(isotope, size_detection_limit) %>% rename(c(detection_limit = size_detection_limit)) %>% mutate(parameter = "mean_size")
)

# fit lm for each spike level, parameter (pnc/mass/size) ~ day.
# comment: Warning in code is due to sample without replicates.

poster_precision <-
  poster_processed %>%
  mutate(
    day = as.factor(day)
  ) %>%
  filter(str_detect(sample_group, "^\\w$")) %>%
  mutate(sample_group = case_when(
    str_detect(sample_group, "A|B|C") ~ "Farm",
    str_detect(sample_group, "G|H|I") ~ "Harbor",
    str_detect(sample_group, "J|K|L") ~ "Surveillance"
  )) %>%
  select(-n_particles, -transport_efficiency) %>%
  pivot_longer(
    cols = c(mean_size, mass_conc, particle_conc),
    names_to = "parameter",
    values_to = "value"
  ) %>%
  group_by(parameter, isotope, sample_group) %>%
  nest() %>%
  ungroup() %>%
  mutate(
    anovas = map(
      data,
      ~ .x %>%
        lm(formula = value ~ day) %>%
        anova() %>%
        tidy() %>%
        select(term, sumsq, meansq) %>%
        pivot_wider(names_from = term, values_from = c(meansq, sumsq))
    ),
    grand_means = map(data, ~ .x %>%
      ungroup() %>%
      summarise(grand_mean = mean(value))),
    rsd = map(data, ~ .x %>%
      ungroup() %>%
      summarise(rsd = sd(value) / mean(value))),
  ) %>%
  unnest(anovas, grand_means, rsd) %>%
  # Precision parameters
  mutate(
    RSE_repeatability = sqrt(meansq_Residuals) / grand_mean,
    RSE_intermediate_precision = sqrt(
      sqrt(meansq_Residuals)^2 +
        sqrt(
          (meansq_day - meansq_Residuals) / 3
          #
        )^2
    ) / grand_mean
  ) %>%
  left_join(poster_dls, by = join_by(isotope, parameter))


# size table
poster_precision %>% filter(parameter == "mean_size") %>% select(isotope, sample_group, isotope, grand_mean, RSE_repeatability,RSE_intermediate_precision, rsd, detection_limit) %>% 
  mutate(RSE_intermediate_precision = if_else(RSE_intermediate_precision %>% is.na(), rsd, RSE_intermediate_precision )) %>% 
  mutate(detection_limit = detection_limit/(10^(-9))) %>% 
  select(-rsd) %>% 
  select(isotope, sample_group, grand_mean, detection_limit, RSE_repeatability, RSE_intermediate_precision) %>% 
  gt(groupname_col = "isotope",
     rowname_col = "sample_group"
    ) %>% 
  cols_label(
    grand_mean = html("Mean [nm]"),
    RSE_repeatability = html("RSD<sub>Repeatability</sub>"),
    RSE_intermediate_precision = html("RSD<sub>Intermediate precision</sub>"),
    detection_limit = html("Detection limit [nm]")
  ) %>% tab_header("Diameter") %>% 
   tab_footnote(
    footnote = md("RSD is presented as variance was lower between days than within days."),
    locations = cells_body(
      columns = vars(RSE_intermediate_precision),
      rows = RSE_intermediate_precision <= RSE_repeatability),
    placement = "left") %>% 
  opt_footnote_marks(marks = "standard") %>% 
  fmt_number(decimals = 0) %>% 
  fmt_percent(columns = matches("rsd|RSE"), decimals = 0) %>% 
  gtsave(filename = "poster_preclod_size.docx")

# mass conc table
poster_precision %>% filter(parameter == "mass_conc") %>% select(isotope, sample_group, isotope, grand_mean, RSE_repeatability,RSE_intermediate_precision, rsd, detection_limit ) %>% 
  mutate(RSE_intermediate_precision = if_else(RSE_intermediate_precision %>% is.na(), rsd, RSE_intermediate_precision )) %>% 
  select(-rsd) %>% 
  select(isotope, sample_group, grand_mean, detection_limit, RSE_repeatability, RSE_intermediate_precision) %>% 
  gt(groupname_col = "isotope",
     rowname_col = "sample_group"
    ) %>% 
  cols_label(
    grand_mean = html("Mean [ng/g]"),
    RSE_repeatability = html("RSD<sub>Repeatability</sub>"),
    RSE_intermediate_precision = html("RSD<sub>Intermediate precision</sub>"),
    detection_limit = html("Detection limit [ng/g]")
  ) %>% tab_header("Mass concentration") %>% 
   tab_footnote(
    footnote = md("RSD is presented as variance was lower between days than within days."),
    locations = cells_body(
      columns = vars(RSE_intermediate_precision),
      rows = RSE_intermediate_precision <= RSE_repeatability),
    placement = "left") %>% 
  opt_footnote_marks(marks = "standard") %>% 
  fmt_number(decimals = 0) %>% 
    fmt_number(columns = matches("detection"), decimals = 1) %>% 
  fmt_percent(columns = matches("rsd|RSE"), decimals = 0) %>% 

  gtsave(filename = "poster_preclod_mass.docx")

  # part conc table
poster_precision %>% filter(parameter == "particle_conc") %>% select(isotope, sample_group, isotope, grand_mean, RSE_repeatability,RSE_intermediate_precision, rsd, detection_limit ) %>% 
  mutate(RSE_intermediate_precision = if_else(RSE_intermediate_precision %>% is.na(), rsd, RSE_intermediate_precision )) %>% 
  select(-rsd) %>% 
  select(isotope, sample_group, grand_mean, detection_limit, RSE_repeatability, RSE_intermediate_precision) %>% 
  gt(groupname_col = "isotope",
     rowname_col = "sample_group"
    ) %>% 
  cols_label(
    grand_mean = html("Mean [#/g]"),
    RSE_repeatability = html("RSD<sub>Repeatability</sub>"),
    RSE_intermediate_precision = html("RSD<sub>Intermediate precision</sub>"),
    detection_limit = html("Detection limit [#/g]")
  ) %>% tab_header("Particle concentration") %>% 
   tab_footnote(
    footnote = md("RSD is presented as variance was lower between days than within days."),
    locations = cells_body(
      columns = vars(RSE_intermediate_precision),
      rows = RSE_intermediate_precision <= RSE_repeatability),
    placement = "left") %>% 
  opt_footnote_marks(marks = "standard") %>% 
  fmt_number(decimals = 0) %>% 
  fmt_percent(columns = matches("rsd|RSE"), decimals = 0) %>% 
  fmt_scientific(
              columns = c(3,4),
              decimals = 1) %>% 
  gtsave(filename = "poster_preclod_pnc.docx")

```

```{r}
# poster_table %>% gt() %>%
#   fmt_number(
#     columns = 3:4,
#     decimals = 2
#   ) %>%
# fmt_scientific(
#               columns = 5,
#               decimals = 2) %>% 
#   gtsave(filename = "poster_table.docx")
```
## Contamination from homogenization

```{r}
contamination_prpoly_noinc <- sp_classifier("~/sp-data/201017_MultiMe_prelim_prbnopmex_2308poster_reanalysis.b/") %>%
  filter(isotope %in% c("Ti", "Cu", "Cr")) %>%
  mutate(rawdata = future_map(filepath, ~ sp_reader(.) %>%
    mutate(time = row_number() / 10000))) %>%
  unnest(rawdata) %>% 
  mutate(
    
        sample_name = case_when(
          sample_name == "PrB no pmex" ~ "Homogenized UPW",
          sample_name == "PrPoly no pmex" ~ "Homogenized PE",
          TRUE ~ "WRONG SAMPLE"
        )) %>% 
  mutate(
    sample_name = factor(sample_name, levels = c(
      "Homogenized UPW",
      "Homogenized PE"
    )))

cont_plot <- contamination_prpoly_noinc %>%
  # filter(isotope == "Cu", sample_name == "PrB no pmex") %>%
  ggplot(aes(x = time, y = counts)) +
  geom_line() +
  facet_grid(sample_name ~ isotope)+
  # scale_y_continuous(breaks = breaks_pretty(n=6, min.n = 4), labels = fancy_scientific)+
  xlab("Counts [#/100\u03BCs]") +
  ylab("Time [s]")



ggsave(cont_plot, filename = "cont_plot.tiff", width = 10, height = 5.25 )

```

## Artifact

```{r}
tar_read(au_sizes) %>%
  filter(
    sample_name %>% str_detect("Au RM"),
    !str_detect(sample_name, "ERROR|200")
  ) %>%
  unnest(peaks) %>%
  mutate(
    sample_name = factor(sample_name, levels = c(
      "Au RM 10.9",
      "Au RM 20.3",
      "Au RM 30",
      "Au RM 39",
      "Au RM 51",
      "Au RM 63",
      "Au RM 83",
      "Au RM 100"
    ))
  ) %>%
  group_by(sample_name) %>%
  mutate(particle_size_ref = str_extract(sample_name, "[\\d\\.]+$") %>% as.numeric()) %>%
  ggplot(aes(particle_size)) +
  geom_vline(aes(xintercept = particle_size_ref), color = "black", linetype = "longdash") +
  geom_histogram(position = "identity", binwidth = 0.5, alpha = 0.6) +
  facet_wrap(~sample_name, scale = "free", ncol = 2) +
  xlim(c(0, 120)) +
  theme_linedraw(base_size = 15) +
  theme(panel.grid.major = element_line(size = 0.05)) +
  theme(panel.grid.minor = element_line(size = 0.05)) +
  scale_fill_manual(values = c("#436EEE", "#8B3626")) +
  xlab("Particle diameter [nm]") +
  ylab("Frequency [#]")
ggsave(filename = "artifat_au_sizes.tiff", width = 15, height = 18)

```

---
title: "Au validation"
output:
  bookdown::word_document2:
    latex_engine: xelatex
    theme: journal
    code_folding: hide
    number_sections: false
editor_options:
  markdown:
    wrap: 150
always_allow_html: yes
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  echo = TRUE,
  warning = FALSE
)

library(targets)
library(multcomp)
library(tidyverse)
library(kableExtra)
library(broom)
library(patchwork)
library(see)
library(metRology)
library(scales)

# library(flextable)
library(gt)
source("~/Documents/GitHub/sp_development/r/sp_funs_equal_h_thr.R")
theme_set(
  theme_linedraw(base_size = 14) +
  theme(
    panel.grid.major = element_line(size = 0.05),
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text.x = element_text(face = "bold")
  ) +
  theme(panel.grid.minor = element_line(size = 0.05))
)
```

## Resubmission JAGF

### Reviewer 1

Refer to Katrin email to see questions, Reviewer 1.

0.  Higlight novel points vs existing studies: Precision, enzyme, algorithm: ARE
1.  Rewrite highlights - same as abstract. (check if we really need highlights in this JAGF? ARE
2.  Aggregation problem, not addressed: ARE
    -   Discuss how stability may be changed
    -   Appear not to agglomerate: Show that sample prep does not induce aggl.
    -   Cannot exclude the possibility that natural particles behave diff and aggr, or are already agglomerated.
3.  Why protamex: Andre + are send file
    -   Prize: Calculate cost per sample
    -   Make sure Robustness, Simplcity, Non-hazardous is highlighted and:
    -   Argue for novelty (in introd, vs other papers use either ..)
4.  Specificity ti vs tio2: 
    -   Mention, Ti-based particles. Mass equivalent diameter for Ti.
    -   Check that density data is present in MS and that assumption is clear.
5.  Ceramic tools Cr
    -   Kitchen machine, mention.
    -   See if data on cont of Cr is present in poster data.
    -   Just show the data, qualitative. Or quantitative. "To exclude the possibility of Cr contamination ..." ADD DATA.
    -   Maybe Andre has some data in microscope?
6.  Methods:
    -   How introduced: amount, time
    -   Clarify in methods that the method applicability demonstration is for Ti, Cu, Cr.
7.  Reagent blank
    -   Change name in MS and tables
    -   Check defeinition of method blank. Dont change name for now.
8.  Selectivity not adressed esp for Cr.
    -   Mention interferences for Cr, comment if OK. Polyatomic only background, not relevant for particles
    -   Check if we have background. If not background, highly unlikely we have "polyatomic" particles
9.  particle diameter, particle mass concentration and particle number concentration of unspiked mussel were not "non-detected". Can such mussel
    really contain so many gold NPs? Please clarify.
    -   Remove low samples, exchange w \<LOD in table 1 and text if relevant.
10. Ignore. Or reinforce that dilution is nice for SP-ICP-MS except if you have contamination issues.
11. Dl Au 1.7 ng/g. Recovery range 40 - 120 at this level?
    -   Check that we compared recoveries with other studies.
    -   Include any regulatory limits?
    -   Add statment that no criteria for NPs, as such we use codex for relevant levels.
12. How about validation for Cr, Cu, Ti?
    -   Ignore?
13. Surveillance sample better homogeneity?
    -   Explain that this is due to diff texture of mussels
    -   More heterogenous sample?
14. Add the digit so that not 0.
15. New paper: An Approach Based on an Increased Bandpass for Enabling the Use of Internal Standards in Single Particle ICP-MS: Application to AuNPs
    Characterization."Please cite"

-   Add IS to avoid high dilution factor?
-   Not relevant?

### Reviewer 2

1.  Change wording to "avoid" or similar.
2.  Please describe the significance of the selection of blue mussels and the innovation point of this work, we just change matrix
    -   Composition very important
    -   Blue mussels important because..
    -   Strengthen highlights as mentioned w rev 1, n0.
    -   
3.  Look for data on spiking Au to mussels diluted 500x or check that no increase in ionic background and add sentence about this. And check if higher
    dilution mussels have been spiked with gold.
      - Are and Andre
4.  It is well-established that using "expert judgement" is common practice. Reference GUM (Stig).
    - Stig reference GUM?

### Reviewer 3

1.  Reference code in github and specify further for "threshold was calculated based on the assumption that the background noise followed a Poisson
    distribution, with a constraint of a 95% probability of observing no more than one false positive per minute". How was this determined?
    Poisson-normal or exact?
    - Are

2.  Explain high number of particles in blanks for Au. Carryover.
    - Andre

3.  Artifact: explain that not due to split events or processing of dissolved background. It happens when NO background, if background not relevant
    anymore.
      - Are

4.  Add references on matrix matching as a way to overcome, including own paper (:D)
    - Refer to own paper, Are.

5.  Matrix effect on Ti, Cu, Cr at 5k dilution. Have any data? Mention that 1/10th of matrix.
    - Are/ANDRE

6.  "The authors need to prove that the Cu NPs are not really just TiO signals recorded from large Ti-containing particles".
    -   Andre/ARE refer to oxide formation percentages max in Agilent 7900.
    -   Are/Andre find data with spiked TiO2 and check that no Cu particles.
    -   How large copper particle can a Ti-particle be detected as given interference.

## Introduction

## Data processing

## Data cleaning

```{r cleaning}
au_alldays_cleaned <- tar_read(au_alldays) %>%
  filter(
    !str_detect(sample_name, "Au\\+")
  ) %>%
  mutate(
    replicate = if_else(
      str_detect(sample_name, "^\\dB") | str_detect(sample_name, "^\\sB") | str_detect(sample_name, "Ringtest B"),
      2, 1
    ),
    sample_group = sample_name,
    sample_group = case_when(
      str_detect(sample_name, "matriseblank") ~ "Matrix blank",
      str_detect(sample_name, "blåskjell") ~ "Blue mussel",
      str_detect(sample_name, "metodeblank") ~ "Method blank",
      str_detect(sample_name, "metodeblank") ~ "Method blank",
      str_detect(sample_name, "Ringtest") ~ "Ringtest",
      str_detect(sample_name, "RM") ~ "Instrument blank",
      str_detect(sample_name, "vann") ~ "Instrument blank",
      TRUE ~ sample_group
    ),
    spike = case_when(
      str_detect(sample_name, "30\\s\\+\\s60") ~ "30 and 60 nm",
      str_detect(sample_name, "30") ~ "30 nm",
      str_detect(sample_name, "60|Ringtest") ~ "60 nm",
      TRUE ~ ""
    )
  ) %>%
  select(day, sample_group, spike, replicate, mean_size, mass_conc, particle_conc, n_particles, transport_efficiency, mass_thr, mass_signal, peaks) %>%
  # add replicate # to water samples aka instrument blank samples using join.
  group_by(day, sample_group, spike) %>%
  mutate(
    replicate = if_else(str_detect(sample_group, "Instrument blank"), cumsum(replicate), replicate)
  ) %>%
  # remove André's imported duplicates
  distinct(day, sample_group, spike, replicate, .keep_all = TRUE) %>%
  # remove additional water sample in last day
  filter(!(replicate == 7 & sample_group == "Instrument blank")) %>%
  ungroup()

write_excel_csv(au_alldays_cleaned %>% select(-peaks), file = paste0(Sys.Date(), "_au_alldays_cleaned.csv"))

```

## Au overview

```{r sample-tables}
# day on x-axis, sample on y. One table for each parameter.
# units is ng/kg mussel tissue, or particles per kg mussel tissue.
validation_parameters <- list("particle_conc", "mass_conc", "mean_size")

gt_tables_list <- validation_parameters %>%
  map(
    .f = ~ {
      validation_parameter <- .x

      gt_table <- au_alldays_cleaned %>%
        # change to ng/kg mussel tissue from ng/L in measured aliquot
        mutate(mass_conc = mass_conc/2,
               particle_conc = particle_conc/2) %>% 
        filter(sample_group %in% c("Blue mussel", "Method blank", "Instrument blank", "Matrix blank", "Deionized water")) %>%
        filter(str_detect(spike, "nm")) %>% 
        select(day, sample_group, spike, replicate, any_of(validation_parameter)) %>%
        pivot_wider(names_from = day, values_from = validation_parameter) %>%
        unite(
          "Sample", sample_group, spike,
          sep = " + "
        ) %>%
        arrange(desc(Sample)) %>%
        mutate(Sample = str_remove_all(Sample, "[\\+\\s]{1,2}$")) %>%
        # rename_with("Blue mussel", !matches("nm")) %>%
        gt(
          rowname_col = "replicate",
          groupname_col = "Sample"
        )

      {
        if (validation_parameter == "particle_conc") {
          gt_table %>%
            tab_header(title = "Particle number concentration") %>%
            tab_spanner(
              label = "Day",
              columns = c(3, 4, 5, 6, 7)
            ) %>%
            tab_stubhead(label = "Sample/ Replicate") %>%
            fmt_scientific(
              columns = 3:7,
              decimals = 2
            )
        } else if (validation_parameter == "mass_conc") {
          gt_table %>%
            tab_header(title = "Particle mass concentration") %>%
            tab_spanner(
              label = "Day",
              columns = c(3, 4, 5, 6, 7)
            ) %>%
            tab_stubhead(label = "Sample/ Replicate") %>%
            fmt_number(
              columns = 3:7,
              decimals = 2
            )
        } else if (validation_parameter == "mean_size") {
          gt_table %>%
            tab_header(title = "Mean particle diameter") %>%
            tab_spanner(
              label = "Day",
              columns = c(3, 4, 5, 6, 7)
            ) %>%
            tab_stubhead(label = "Sample/ Replicate") %>%
            fmt_number(
              columns = 3:7,
              decimals = 2
            )
        } else {
          print("Error in gt_table formatting function.")
        }
      }
    }
  )

gt_tables_list[[1]]

gt_tables_list[[2]]

gt_tables_list[[3]]

```

```{r}
gt_tables_list[[1]] %>% gtsave(filename = "pnc_table.docx")

gt_tables_list[[2]] %>% gtsave(filename = "mc_table.docx")

gt_tables_list[[3]] %>% gtsave(filename = "ms_table.docx")
```

## Precision

```{r ANOVA-precision}

# fit lm for each spike level, parameter (pnc/mass/size) ~ day.
# comment: Warning in code is due to sample without replicates.

precision_anovas <-
  au_alldays_cleaned %>%
  mutate(
    day = as.factor(day),
    replicate = as.factor(replicate)
  ) %>%
  filter(sample_group == "Blue mussel") %>%
  select(-n_particles, -transport_efficiency) %>%
  pivot_longer(
    cols = c(mean_size, mass_conc, particle_conc),
    names_to = "parameter",
    values_to = "value"
  ) %>%
  group_by(spike, parameter) %>%
  nest() %>%
  ungroup() %>%
  mutate(
    anovas = map(
      data,
      ~ .x %>%
        lm(formula = value ~ day) %>%
        anova() %>%
        tidy() %>%
        select(term, sumsq, meansq) %>%
        pivot_wider(names_from = term, values_from = c(meansq, sumsq))
    ),
    grand_means = map(data, ~ .x %>%
      ungroup() %>%
      summarise(grand_mean = mean(value)))
  ) %>%
  unnest(anovas, grand_means) %>%
  # Precision parameters
  mutate(
    RSD_repeatability = sqrt(meansq_Residuals) / grand_mean,
    RSD_intermediate_precision = sqrt(
      sqrt(meansq_Residuals)^2 +
        sqrt(
          (meansq_day - meansq_Residuals) / 2
          # NOTE THAT THIS DOES NOT GENERALISE, ONLY VALID IF TWO REPLICATES
        )^2
    ) / grand_mean
  )


precision_gtable <- precision_anovas %>%
  filter(str_detect(spike, "\\d")) %>%
  select(spike, parameter, RSD_repeatability, RSD_intermediate_precision) %>%
  rename(c(sample = spike)) %>%
  mutate(parameter = case_when(
    parameter == "mass_conc" ~ "Particle mass concentration",
    parameter == "particle_conc" ~ "Particle number concentration",
    parameter == "mean_size" ~ "Mean size"
  )) %>%
  mutate(sample = paste("Blue mussel +", sample)) %>%
  gt(
    rowname_col = "sample",
    groupname_col = "parameter"
  ) #%>%
  # fmt_percent(columns = -c(1, 2)) %>%
  # cols_label(
  #   RSE_repeatability = html("RSD<sub>Repeatability</sub>"),
  #   RSE_intermediate_precision = html("RSD<sub>Intermediate precision</sub>")
  # )

precision_gtable

precision_gtable %>% gtsave(filename = "precision_table.docx")
  
```

## Trueness

\### As recovery

```{r trueness-vs-measured, include = FALSE}
trueness_meas_gt <- au_alldays_cleaned %>%
  filter(
    sample_group %in%
      c(
        "Blue mussel",
        "Matrix blank",
        "Method blank",
        "Instrument blank"
      ),
    spike %in% c("30 nm", "60 nm")
  ) %>%
  group_by(sample_group, spike) %>%
  summarise(across(c("particle_conc", "mass_conc", "mean_size"), ~ mean(.))) %>%
  group_by(spike) %>%
  mutate(across(c("particle_conc", "mass_conc", "mean_size"), ~ .x / .x[sample_group == "Instrument blank"])) %>%
  select(sample_group, spike, mean_size, mass_conc, particle_conc) %>%
  gt() %>%
  fmt_percent(
    columns = !matches("sample_group"),
    decimals = 0
  ) %>%
  cols_label(
    mean_size = "Particle diameter",
    mass_conc = "Particle mass concentration ",
    particle_conc = "Particle number concentration",
    sample_group = "Sample"
  )

trueness_meas_gt %>%
  gtsave(filename = "trueness_meas_rm_gt.docx")

trueness_meas_gt
  
```

```{r trueness-vs-theoretical}
# NPC: 4.58*10^7 or 3.66*10^8.
# Str: 30 og 60.
# Mass conc: 100 ng/L.

trueness_theo_gt <- au_alldays_cleaned %>%
  filter(
    sample_group %in%
      c(
        "Blue mussel",
        "BM spike after digest",
        "Method blank",
        "RM"
      ),
    spike %in% c("30 nm", "60 nm")
  ) %>%
  group_by(sample_group, spike) %>%
  summarise(across(c("particle_conc", "mass_conc", "mean_size"), ~ mean(.))) %>%
  group_by(spike) %>%
  mutate(
    particle_conc = if_else(spike == "30 nm", particle_conc / (3.66*10^8) , particle_conc / (4.58*10^7)),
    mass_conc = mass_conc/100,
    mean_size = if_else(spike == "30 nm", mean_size / 30 , mean_size / 60)
  ) %>% 
  select(sample_group, spike, mean_size, mass_conc, particle_conc) %>% 
  gt() %>% 
    fmt_percent(
    columns = !matches("sample_group"),
    decimals = 0
  ) %>% 
     cols_label(
    mean_size = "Particle diameter",
    mass_conc = "Particle mass concentration ",
    particle_conc = "Particle number concentration"
  )

trueness_theo_gt

trueness_theo_gt %>%  gtsave(filename = "trueness_theo_gt.docx")
  
```

### From proficiency test

```{r}
au_alldays_cleaned %>% filter(sample_group == "Ringtest") %>%
  summarise(across(c("particle_conc", "mean_size"), ~ mean(.))) %>%
  mutate(
    particle_conc_recovery = particle_conc/(1.44*10^8),
    mean_size_recovery = mean_size/61
    
  ) %>% 
  mutate(zscore_pnc = (particle_conc - (1.44*10^8)) / (0.65*10^8),
         zscore_size = (mean_size - 61) / 6.1 ) %>% gt() %>% fmt_number(columns = -particle_conc,decimals = 2)

```

## Selectivity

-   Is there interferences on the measurement of the analyte of interest?
    -   Presence in blank.
    -   Change in spiked signal.

### Table

```{r}

au_alldays_cleaned %>%
  mutate(
    mass_conc = mass_conc / 2,
    particle_conc = particle_conc / 2
  ) %>%
  filter(
    str_detect(sample_group, "blank"),
    str_detect(sample_group, "Method", negate = TRUE)
  ) %>%
  group_by(
    sample_group, spike
  ) %>%
  summarise(
    across(
      .cols = c(mean_size, mass_conc, particle_conc),
      list(
        mean = ~ mean(.x, na.rm = TRUE),
        sd = ~ sd(.x, na.rm = TRUE)
      )
    )
  ) %>% 
  gt(rowname_col = "spike") %>%
  fmt_number(
    columns = 3:6,
    decimals = 1
  ) %>%
  fmt_scientific(
    7:8,
    rows = everything(),
    decimals = 2,
    drop_trailing_zeros = FALSE,
    scale_by = 1,
    pattern = "{x}",
    sep_mark = ",",
    dec_mark = ".",
    # force_sign = FALSE,
    locale = NULL
  ) %>%
  cols_merge_uncert(mean_size_mean, mean_size_sd, sep = " ± ", autohide = TRUE) %>% 
  cols_merge_uncert(mass_conc_mean, mass_conc_sd, sep = " ± ", autohide = TRUE) %>% 
  cols_merge_uncert(particle_conc_mean, particle_conc_sd, sep = " ± ", autohide = TRUE) %>% 
   cols_label(
    mean_size_mean = "Particle diameter",
    mass_conc_mean = "Particle mass concentration",
    particle_conc_mean = "Particle number concentration"
  ) %>% 

  gtsave(filename = "selectivity_table.docx")
  
```

```{r}
# plot particle size distribution of 30nm and 60 nm side by side, superimposing in instrument blank and in matrix blank.

plot_selectivity <- au_alldays_cleaned %>% 
  filter(
    str_detect(sample_group, "blank"),
    str_detect(sample_group, "Method", negate = TRUE),
    spike != ""
  ) %>% 
  ungroup() %>%
  mutate(sample_group = case_when(
    sample_group == "Instrument blank" ~ "Spiked UPW",
    sample_group == "Matrix blank" ~ "Spiked mussel tissue",
    TRUE ~ NA
  )) %>% 
  unnest(peaks) %>% group_by(sample_group, spike) %>% 
  mutate(particle_size_mean = mean(particle_size, na.rm = TRUE)) %>% 
  ggplot(aes(particle_size,
    fill = sample_group
  )) + geom_vline(aes(xintercept = particle_size_mean), color = "red", linetype = "longdash")+
  geom_histogram(position = "identity", binwidth = 0.5, alpha = 0.6) +
  facet_wrap(~spike, scale = "free") +
  
  xlim(c(0, 75))+
  theme_linedraw(base_size = 15)+
     theme(panel.grid.major = element_line(size = 0.05))+
  theme(panel.grid.minor = element_line(size = 0.05))+
  scale_fill_manual(values=c("#436EEE", "#8B3626"))+
  xlab("Particle diameter [nm]")+
  ylab("Frequency [#]")


plot_selectivity

# showing artifact peak max distribution. These could be filtered by changing detection threshold.
# au_alldays_cleaned %>%
#   filter(
#     str_detect(sample_group, "blank"),
#     str_detect(sample_group, "Method", negate = TRUE),
#     spike != ""
#   ) %>%
#   ungroup() %>%
#   unnest(peaks) %>%
#   filter(spike == "60 nm",
#          particle_size < 25) %>% ggplot(aes(peak_max/peak_width))+geom_histogram(binwidth =0.5)+theme_bw()

ggsave(plot_selectivity, filename = "plot_selectivity.tiff", width = 15, height = 5 )
```

### Statistics

```{r}
# Instrument blank vs matrix blank (interfering species present)
# Sub-ng concentration, higher conc in mussel (avg 25 vs 15).
# Statistical significance driven by outlier in matrix blank.

list("mass_conc", "n_particles") %>%
  map(
    ~ {
        t.test(
          au_alldays_cleaned %>%
            filter(
              sample_group == "Instrument blank",
              spike == ""
            ) %>% pull(.x),
          au_alldays_cleaned %>%
            filter(
              sample_group == "Matrix blank",
              spike == "",
              day != 1
            ) %>% pull(.x)
        )
    }
  )

# Instrument blank spiked vs matrix blank spiked (matrix effects)

# mass_conc, particle conc, mean size
# 30 nm
list("mass_conc", "particle_conc", "mean_size") %>%
  map(
    ~ {
        wilcox.test(
          au_alldays_cleaned %>%
            filter(
              sample_group == "Instrument blank",
              spike == "30 nm"
            ) %>% pull(.x),
          au_alldays_cleaned %>%
            filter(
              sample_group == "Matrix blank",
              spike == "30 nm"
            ) %>% pull(.x)
        )
    }
  )


# 60 nm
list("mass_conc", "particle_conc", "mean_size") %>%
  map(
    ~ {
        t.test(
          au_alldays_cleaned %>%
            filter(
              sample_group == "Instrument blank",
              spike == "60 nm"
            ) %>% pull(.x),
          au_alldays_cleaned %>%
            filter(
              sample_group == "Matrix blank",
              spike == "60 nm"
            ) %>% pull(.x)
        )
    }
  )

# sensitivity via particle mass

# 30 nm

        t.test(
          au_alldays_cleaned %>% unnest(peaks) %>% 
            filter(
              sample_group == "Instrument blank",
              spike == "30 nm"
            ) %>% pull(particle_mass),
          au_alldays_cleaned %>% unnest(peaks) %>% 
            filter(
              sample_group == "Matrix blank",
              spike == "30 nm"
            ) %>% pull(particle_mass)
        )
1-3.910639e-19/4.829401e-19

# 60 nm

        t.test(
          au_alldays_cleaned %>% unnest(peaks) %>% 
            filter(
              sample_group == "Instrument blank",
              spike == "60 nm"
            ) %>% pull(particle_mass),
          au_alldays_cleaned %>% unnest(peaks) %>% 
            filter(
              sample_group == "Matrix blank",
              spike == "60 nm"
            ) %>% pull(particle_mass)
        )
1-1.788208e-18/2.274148e-18

```

We have a marginal significant difference between unspiked matrix blank and instrument blank in terms of particle mass or particle number
concentrations. Yet, this in both cases sub-ng/L and mostly driven by single outlier. Hence, no interfering species appear to be present.

We have a significant difference between matrix blank instrument blank in terms of particle mass concentration and mean particle size, but not
particle number concentration. This indicates that we do not have a change in transport efficiency, yet in the signal per particle.

```{r}
# #pragmatic/preliminary for presentation
# tar_read(au_alldays) %>% filter(str_detect(sample_name, "60 nm Au 100 ng/l RM|2B blåskjell 60 nm Au 100 ng/l")) %>% 
# rowwise() %>% unnest(peaks) %>% group_by(sample_name, day) %>% 
#   summarise(median_peak_area = median(peak_area, na.rm = TRUE),
#             mean_particle_size = mean(particle_size, na.rm = TRUE),
#             mean_particle_conc = mean(particle_conc, na.rm = TRUE),
#             mean_mass_conc = mean(mass_conc, na.rm = TRUE)) %>% 
#   group_by(sample_name) %>% 
#   summarise(
#     mean_peak_area = mean(median_peak_area, na.rm = TRUE),
#             mean_particle_size = mean(mean_particle_size, na.rm = TRUE),
#             mean_particle_conc = mean(mean_particle_conc, na.rm = TRUE),
#             mean_mass_conc = mean(mean_mass_conc, na.rm = TRUE)
#   )
# 
# # Size matrix fx: `r 53.4/59.10` signal matrix fx`r 348.8/463.2` pnc sign `r 20567565/20366243` mc `r 35.67223/46.32001`
```

## Working range

```{r}

# Make calibration curves for each day-batch, appending day number.
calibration <- list.files("~/sp-data/paper0_au_validering", full.names = TRUE) %>%
  map_df(.f = ~ sp_classifier(.x, "60 nm Au 100 ng/l RM") %>%
    sp_response() %>%
    mutate(day = str_extract(.x, "\\d(?=.b$)"))) %>%
  distinct(response, .keep_all = TRUE) %>%
  select(day, intercept, response, r2)

calibration %>%
  rename_with(~ .x %>%
    str_to_title()) %>%
  gt() %>%
  fmt_number(
    columns = 2:3,
    decimals = 2
  ) %>%
  fmt_number(
    columns = 4,
    decimals = 5
  ) %>%
  gtsave(filename = "calibration_curves.docx")

```

### Particle number concentration

Assuming the particles are homogeneously dispersed, the number of particles measured in a given interval will follow a Poisson distribution. The rate
parameter, $\lambda$, corresponds to the mean number of particles in this interval. It follows from number theory that the interarrival times of such
events will follow an exponential distribution with rate parameter $\lambda$.

We have that $\lambda = c_n \cdot \frac{Q_{det}}{t_{meas}}$, where $c_n$ is the particle number concentration of the measured solution in particles/L,
$Q_{det}$ is the volumetric mass transport through the ICP in L per total acquisition time and $t_{meas}$ is the number of dwells per acquisition
time.

Using the cumulative exponential distribution we obtain

$$F(w_p)=\left\{\begin{aligned} 0, & \text { if } w_p<0 \\ 
1-e^{\frac{-c_n \cdot Q_{det} \cdot w_p}{t_{meas}}}, 
& \text { if } w_p \geq 0\end{aligned}\right.$$

Thus, accepting 5% particle coincidence or overlap, we have:

$$\frac{t_{meas} \cdot \ln{(0.95^{-1})}}{w_p \cdot Q_{det}} = c_n.$$

Inserting the flow rate through the plasma and peak widths obtained for 60 nm:

$c_n = \frac{\ln \left(\frac{1}{0.95}\right)}{\left(12.63 \times 3.407 \times 10^{-11}\right)}$ = $1.2{\cdot 10^8}$ particles per liter.

And similarly for 30 nm:

$c_n = \frac{\ln \left(\frac{1}{0.95}\right)}{\left(9.670 \times 3.407 \times 10^{-11}\right)}$ = $1.55{\cdot 10^8}$ particles per liter.

Taking dilution factor into account, we get for 60 nm `r 500*log(1/0.95)/(12.62884*3.407108*10^-11)` particles per kg mussel tissue and 30 nm
`r 500*log(1/0.95)/(9.699962*3.407108*10^-11)` particles per kg mussel tissue.

## Detection limit

### Critical size

A peak width of 500 $\mu$s is assumed based on approximation of peak widths near the LOD and literature (Laborda 2020). Assuming a triangular peak
shape, it is possible to obtain an estimate for the corresponding particle mass, denoted as $m_p$:

$$m_{p} = \frac{ A_p}{ S } = \frac{w_p \cdot h_p}{2S},$$

where $A_p$ is the particle signal, S is the sensitivity in counts per kg element. Using peak width 5 and height 6, we get $$5*6*1.698164e-21/2$$
gives 2.547246e-20 for the peak mass at LOD.

using $$d = (\frac{6m}{\pi p})^{1/3}$$ we get

```{r peak-width}
peak_width <- 5

#size/mass detection limit
dl_sizemass <- au_alldays_cleaned %>%
  filter(sample_group == "Matrix blank") %>%
  filter(spike == "") %>%
  unnest(peaks) %>%
  group_by(day) %>%
  summarise(h_thr = median(h_thr, na.rm = TRUE),
            mass_thr = median(mass_thr, na.rm = TRUE)) %>%
  mutate(
    mass_detection_limit = peak_width * mass_thr/2,
    size_detection_limit = ((6*mass_detection_limit)/(pi*19.32*1000))^(1/3)
  )
    
```

### Concentrations

```{r}

#particle conc detection limit, using pooled blanks and mean number of particles detected. Converted to pnc by using the mean volume flow. Including dilution factor.
  
 dl_pnc <- au_alldays_cleaned %>%
   filter(str_detect(sample_group, "blank")) %>%
   filter(spike == "") %>% # group_by(day) %>%
   summarise(
     pnc_dl_pois =  poisson.test(mean(n_particles) %>% round(digits = 0), conf.level = 0.997)$conf.int[2] * mean(particle_conc) / (2*mean(n_particles)),
     pnc_dl_gaussian =  (mean(n_particles) + 3 * sd(n_particles)) * mean(particle_conc) / (2*mean(n_particles)),
     mass_conc_dl_gaussian =  (mean(mass_conc) + 3 * sd(mass_conc))/2
   )
 
```

## Uncertainty

To determine the signal per element mass, $S$, a particulate reference material with a known mass or size can be analyzed. By integrating
background-subtracted peaks and using the median or the kernel density estimate of the resulting signal distribution, we obtain an estimate of the
signal per particle mass or size in [counts/kg]:

$$S =  \frac{6 A_{rm}}{\rho  \cdot \pi \cdot d_{rm}^3},$$

where $\rho$ is the element density, $d_{rm}$ the particle diameter and $A_{rm}$ is the estimated signal per particle in counts.

Using ionic standards to establish a calibration curve, we use the slope coefficient, $b$, and the concentration of the corresponding ionic standard,
$C_{std}$, for an estimate for the signal per dwell per mass element per volume in $\frac{counts}{dwell}/ \frac{kg}{L}$:

$$I = \frac{b}{C_{std}}.$$

Combining these and multiplying by $t_{meas}$, the number of dwells during acquisition, the flow rate through the plasma, $Q_{det}$, in
$\frac{L}{t_{acq}}$ may be determined and used to relate measured quantities to concentrations:

$$Q_{det} = \frac{t_{meas} \cdot I}{S}. $$

Mean particle diameter is then given by

$$ \overline d_{p} =  \frac{1}{n}\sum_{i=1}^nd_{p_i} =
\frac{1}{n}\sum_{i=1}^n\sqrt[3]{\frac{6  S \cdot A_{p_i}}{\pi \rho} } = 
\frac{d_{rm}}{n\cdot \sqrt[3]{A_{rm}}}\sum_{i=1}^n\sqrt[3]{A_{p_i} },$$

the particle mass concentration, $c_m$, by

$$c_m = \frac{\sum_{i=1}^nA_{p_i} \cdot S_{}}{Q_{det}} = \frac{ C_{std} \sum_{i=1}^nA_{p_i}}{t_{meas} \cdot b}, $$

and the particle number concentration, $c_n$, by

$$c_{n} = \frac{n_{meas}}{Q_{det}}.$$

Inserting for $Q_{det}$, we get

$$ c_{n}  = \frac{n_{meas} \cdot 6 A_{rm} \cdot C_{std}}{t_{meas} \cdot \rho  \cdot \pi \cdot d_{rm}^3 \cdot b} $$

-   $n_{meas}$ is the instrument measured number of particle events. This corresponds to a Poisson process and hence the standard uncertainty will be
    its square root. Using the LOD for particle number concentration based on 35 measured particles justifies a normal approximation as a conservative
    estimate for the uncertainty.

-   $A_{rm}$ in counts is the location of the peak area per particle event of the reference material as determined using a kernel density estimate,
    uncertainty set to 5%.

-   $A_p$ in counts is the peak area or total signal of a particle event of which 5 % uncertainty is assumed.

-   $C_{std}$ [kg/L] is the concentration of the standard. We use the uncertainty of the standard at 1%, and add another multiplicative term with 5%
    to account for the uncertainty of preparation.

-   $t_{meas}$ is the number of dwells per measurement, uncertainty set to 0.5%.

-   $\rho$ [kg/m\^3] is the element density of a particle. Its uncertainty is not well-established, yet [@montoro_bustos_evaluation_2022] 0.05%. We
    set 1%.

-   $d$ [m] is the particle diameter. For the reference material, the uncertainty in the certificate of analysis at 4 % is used.

-   $b$ [counts/dwell] is the slope coefficient of the calibration curve. We use the uncertainty of the the linear regression on each day.

For each variable the standard uncertainty is assumed for a Monte Carlo simulation with $10^6$ repeats, reporting the uncertainty with a coverage
factor of two.

```{r standard-uncert-pnc}

set.seed(123)
uncertainties <- c(1, 2, 3, 4, 5) %>%
  map_dfr(
    ~ tar_read(au_alldays) %>%
      filter(str_detect(sample_name, "Au\\+|60.*RM"), day == .x) %>%
      {
        d_pip <- 1
        d_pip_su <- 0.05
        n_meas <- 1000 # n particles
        n_meas_su <- sqrt(n_meas)
        t_meas <- 1200000 # n dwells
        t_meas_su <- 1200000 * 0.005
        d_rm <- 60 * 10^-9 # particle diameter reference material
        d_rm_su <- d_rm * 0.04 # standard uncertainty particle diameter reference material
        rho <- 1000 * 19.32 # density Au SI kg/m3
        rho_su <- rho * 0.01 # uncertainty density
        A_rm <- (.) %>%
          filter(sample_name == "60 nm Au 100 ng/l RM") %>%
          unnest(peaks) %>%
          pull(peak_area) %>%
          {
            dens <- density((.), na.rm = TRUE)
            dens$x[which.max(dens$y)]
          } # KDE
        A_rm_su <- A_rm * 0.05
        A_p <- (.) %>%
          filter(sample_name == "60 nm Au 100 ng/l RM") %>%
          unnest(peaks) %>%
          pull(peak_area) %>%
          mean(na.rm = TRUE)
        A_p_su <- A_p * 0.05
        A_psum <- (.) %>%
          filter(sample_name == "60 nm Au 100 ng/l RM") %>%
          unnest(peaks) %>%
          pull(peak_area) %>%
          sum(na.rm = TRUE)
        A_psum_su <- A_psum * 0.05
        pi <- pi
        pi_su <- 0
        R_std <- (.) %>% # response, counts per kg/L
          filter(str_detect(sample_name, "Au\\+")) %>%
          ungroup() %>%
          mutate(
            conc_ppb = str_extract(sample_name, str_extract(
              sample_name,
              paste0("[\\d.]+(?=[\\sa-zA-Z]*", isotope, ")")
            )) %>% as.numeric()
          ) %>%
          mutate(
            intercept =
              coef(
                lm(formula = as.numeric(mean_counts) ~ conc_ppb)
              )[1],
            response =
              coef(
                lm(formula = as.numeric(mean_counts) ~ conc_ppb)
              )[2],
            r2 = summary(
              lm(formula = as.numeric(mean_counts) ~ conc_ppb)
            )$r.squared
          ) %>%
          ungroup() %>%
          pull(response) %>%
          median(na.rm = TRUE)

        R_std_su <- (.) %>% # response, counts per kg/L
          filter(str_detect(sample_name, "Au\\+")) %>%
          ungroup() %>%
          mutate(
            conc_ppb = str_extract(sample_name, str_extract(
              sample_name,
              paste0("[\\d.]+(?=[\\sa-zA-Z]*", isotope, ")")
            )) %>% as.numeric()
          ) %>%
          lm(formula = as.numeric(mean_counts) ~ conc_ppb) %>%
          summary()
        R_std_su <- R_std_su$coefficients[, "Std. Error"][2] %>% as.numeric()

        C_std <- 1 * 10^(-9) # 1 ug/L = 10^-9 kg/L
        C_std_su <- C_std * 0.01
        
    # expression mean diameter
        expr_md <- expression(d_rm * (A_p / A_rm)^(1 / 3))
        # values for mean diameter variables
        x_md <- list(d_rm = d_rm, A_p = A_p, A_rm = A_rm)
        # uncertainties for mean diameter vars
        u_md <- list(d_rm = d_rm_su, A_p = 3*A_p_su/sqrt(n_meas), A_rm = A_rm_su)
        # uncertainties for mean diameter
        u.expr_md <- uncert(expr_md, x_md, u_md, method = "MC", B = 1000000)
        u.expr_md$u.y

        # expression particle number concentration
        expr_pnc <- expression(d_pip * n_meas * (A_p * C_std) / (t_meas * rho * (4 / 3) * pi * (d_rm / 2)^3 * R_std))

        # values for each variable for particle number concentration
        x_pnc <- list(d_pip = d_pip, t_meas = t_meas, n_meas = n_meas, A_p = A_p, C_std = C_std, rho = rho, pi = pi, d_rm = d_rm, R_std = R_std)
        
        # uncertainties for particle number concentration variables
        u_pnc <- list(d_pip = d_pip_su, t_meas = t_meas_su, n_meas = n_meas_su, A_p = A_p_su, C_std = C_std_su, rho = rho_su, pi = pi_su, d_rm = d_rm_su, R_std = R_std_su)
        # uncertaintites for particle number concentration
        u.expr_pnc <- uncert(expr_pnc, x_pnc, u_pnc, method = "MC", B = 1000000)
        u.expr_pnc$u.y

    
        
        # expression particle mass concentration
        expr_pmc <- expression(n_meas*d_pip*C_std*A_psum/(1000*t_meas*R_std))
        # values for pmc variables
        x_pmc <- list(n_meas = n_meas, d_pip = d_pip, C_std = C_std, A_psum = A_psum, t_meas = t_meas, R_std = R_std)
        # uncertainties for pmc vars
        u_pmc <- list(n_meas = n_meas_su, d_pip = d_pip_su, C_std = C_std_su, A_psum = A_psum_su, t_meas = t_meas_su, R_std = R_std_su)
        # uncertainties for pmc
        u.expr_pmc <- uncert(expr_pmc, x_pmc, u_pmc, method = "MC", B = 1000000)
        u.expr_pmc$u.y
list(
        tibble(
          uncertainty_md = (u.expr_md$u.y / (u.expr_md$y)),
          uncertainty_pmc = (u.expr_pmc$u.y / (u.expr_pmc$y)),
          uncertainty_pnc = u.expr_pnc$u.y / u.expr_pnc$y,
          # md=u.expr_md$y,
          # pmc=u.expr_pmc$y,
          # pnc=u.expr_pnc$y,
          day = .x
        )%>% mutate(
  across(starts_with("uncertainty"), ~ .x * 2)
)
# ,
#         plot.uncert(u.expr_md),
#         plot.uncert(u.expr_pmc),
#         plot.uncert(u.expr_pnc)
)
      }
  )

uncertainties %>% 
  summarise(across(everything(), ~ mean(.x))) %>% 
  gt() %>%
  fmt_percent(
    columns = !matches("day"),
    decimals = 1
  ) %>%
  cols_label(
    uncertainty_md = "Mean particle diameter",
    uncertainty_pmc = "Particle mass concentration",
    uncertainty_pnc = "Particle number concentration"
  )
```

## Stability

Particle stability assessed by comparison of spiked blanks versus mussel samples spiked before and after digestion.

```{r}
au30_stab <- au_alldays_cleaned %>%
  filter(
    str_detect(spike, "^\\d{2}\\snm"),
    sample_group %in% c(
      "Method blank",
      "Matrix blank",
      "Blue mussel",
      "Instrument blank"
    ),
    spike == "30 nm"
  ) %>%
  mutate(sample_group = sample_group %>% as.factor()) %>%
  ungroup() %>%
  select(sample_group, mass_conc, particle_conc)

amod_mass_conc <- aov(mass_conc ~ sample_group, data = au30_stab)

amod_particle_conc <- aov(particle_conc ~ sample_group, data = au30_stab)

glht(amod_mass_conc, linfct = mcp(sample_group = "Tukey")) %>% summary()

glht(amod_particle_conc, linfct = mcp(sample_group = "Tukey")) %>% summary()

au60_stab <- au_alldays_cleaned %>%
  filter(
    str_detect(spike, "^\\d{2}\\snm"),
    sample_group %in% c(
      "Method blank",
      "Matrix blank",
      "Blue mussel",
      "Instrument blank"
    ),
    spike == "60 nm"
  ) %>%
  mutate(sample_group = sample_group %>% as.factor()) %>%
  ungroup() %>%
  select(sample_group, mass_conc, particle_conc)

amod_mass_conc <- aov(mass_conc ~ sample_group, data = au60_stab)

amod_particle_conc <- aov(particle_conc ~ sample_group, data = au60_stab)

glht(amod_mass_conc, linfct = mcp(sample_group = "Tukey")) %>% summary()

glht(amod_particle_conc, linfct = mcp(sample_group = "Tukey")) %>% summary()

```

## Poster



```{r}
# LODs, blank calcs:

lod_blk_calculator <- function(processed_batch, acq_time) {
  LODs <- processed_batch %>% select(-matches("_LOD")) %>% 
    select(-peaks) %>% 
    filter(str_detect(sample_name, "PrBlank\\s\\d")) %>%
    group_by(isotope) %>%
    summarise(
      # LODs from procedural blanks
      mass_conc_LOD =  3 * sd(mass_conc),
      total_conc_LOD =  3 * sd(total_conc),
      n_particles_LOD = PoiCI(mean(n_particles), 1 - 0.003),
      particle_conc_LOD = n_particles_LOD / (mean(detector_flow_rate)*1800000),
      # mean blank values from procedural blanks
      blk_mass_conc = mean(mass_conc),
      blk_total_conc = mean(total_conc),
      blk_n_particles = mean(n_particles),
      blk_particle_conc = mean(particle_conc))
  
  return(left_join(processed_batch, LODs, by = "isotope") %>% ungroup() )
}

# Blank subtraction from samples, addition of sample grouping.

subtracter_grouper <- function(processed_batch, acq_time) {
  processed_batch %>%
    filter(isotope != "Au" & type == "SAMPLE") %>%
    lod_blk_calculator(acq_time) %>%
    # sample grouping
    mutate(
      sample_group =
        case_when(
          str_detect(sample_name, "IBLK") ~ "IBLK",
          str_detect(sample_name, "PrBlank\\s\\d") ~ "PMBLK",
          str_detect(sample_name, "^[A-Z]{1}[\\s\\d]{1}") ~ str_extract(sample_name, "^[A-Z]{1}"),
          str_detect(sample_name, "^[A-Z]{1}S[\\s\\d]{1}") ~ str_extract(sample_name, "^[A-Z]{1}S"),
          TRUE ~ NA_character_
        ),
      parallel = str_extract(sample_name, "_p\\d{1,2}") %>% str_extract("\\d{1,2}"),
      measurement = str_extract(sample_name, "_m\\d") %>% str_extract("\\d")
    ) %>%
    # blank subtraction
    group_by(isotope) %>%
    mutate(
      particle_fraction = 100 * mass_conc / (total_conc * 1000),
      mass_conc = if_else(str_detect(sample_name, "x1000"), mass_conc - blk_mass_conc, mass_conc),
      total_conc = if_else(str_detect(sample_name, "x1000"), total_conc - blk_total_conc, total_conc),
      n_particles = if_else(str_detect(sample_name, "x1000"), as.numeric(n_particles - blk_n_particles), as.numeric(n_particles)),
      particle_conc = if_else(str_detect(sample_name, "x1000"), particle_conc - blk_particle_conc, particle_conc)
    ) %>%
    ungroup()
}
```

```{r poster-visualization}

fancy_scientific <- function(l) {
     # turn in to character string in scientific notation
     l <- format(l, scientific = TRUE)
      # fix zero
     l <- gsub("0e\\+00","0",l)
     # quote the part before the exponent to keep all the digits
     l <- gsub("^(.*)e", "'\\1'e", l)
     
          l <- gsub("e\\+","e",l)
     # turn the 'e+' into plotmath format
     l <- gsub("e", "%*%10^", l)

     
     l <- gsub("\\'1[\\.0]*\\'\\%\\*\\%", "", l)
    
     # return this as an expression
     parse(text=l)
}


# change to account for 5000 dilution factor
poster_processed <- rbind(
  tar_read(poster1) %>% subtracter_grouper(acq_time = 180) %>% mutate(day = 1),
  tar_read(poster2) %>% subtracter_grouper(acq_time = 180)%>% mutate(day = 2)
) %>% filter(!str_detect(sample_name, "^[A-Z]{1}[\\d]{1}")) %>%
  sp_comparer(acq_time = 180) %>% 
  mutate(mass_conc = 6 * mass_conc,
         particle_conc = 6 * particle_conc,
         particle_conc_LOD = 6 * particle_conc_LOD,
         mass_conc_LOD = 6 * mass_conc_LOD)

# mass concentration
poster_processed %>%
  filter(
    sample_group != "PMBLK",
    !str_detect(sample_group, "\\wS")
  ) %>%
  mutate(sample_group = case_when(
    str_detect(sample_group, "A|B|C") ~ "Farm",
    str_detect(sample_group, "G|H|I") ~ "Harbor",
    str_detect(sample_group, "J|K|L") ~ "Surveillance"
  )) %>%
  ggplot(aes(sample_group, mass_conc)) +
  stat_summary(geom = "errorbar", fun.data = mean_cl_normal) +
  geom_jitter(width = 0.2, aes(shape = as.factor(day))) +
  geom_hline(
    aes(yintercept = mean(mass_conc_LOD)),
    alpha = .25,
    linetype = "solid",
    color = "red",
    size = 2
  ) +
  facet_wrap(~isotope, scales = "free") +
  theme_linedraw(base_size = 14) +
  theme(
    panel.grid.major = element_line(size = 0.05),
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text.x = element_text(face = "bold")
  ) +
  theme(panel.grid.minor = element_line(size = 0.05)) +
  xlab("") +
  ylab("Particle mass concentration [ng/g]")+
  guides(shape=guide_legend(title="Day"))

ggsave("poster_mass_conc.png", height = 4, width = 9)

#particle conc
poster_processed %>%
  filter(
    sample_group != "PMBLK",
    !str_detect(sample_group, "\\wS")
  ) %>%
  mutate(sample_group = case_when(
    str_detect(sample_group, "A|B|C") ~ "Farm",
    str_detect(sample_group, "G|H|I") ~ "Harbor",
    str_detect(sample_group, "J|K|L") ~ "Surveillance"
  )) %>%
  ggplot(aes(sample_group, particle_conc)) +
  stat_summary(geom = "errorbar", fun.data = mean_cl_normal) +
  geom_jitter(width = 0.2, aes(shape = as.factor(day))) +
  geom_hline(
    aes(yintercept = mean(particle_conc_LOD)),
    alpha = .25,
    linetype = "solid",
    color = "red",
    size = 2
  ) +
  facet_wrap(~isotope, scales = "free") +
  theme_linedraw(base_size = 14) +
  theme(
    panel.grid.major = element_line(size = 0.05),
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text.x = element_text(face = "bold")
  ) +
  theme(panel.grid.minor = element_line(size = 0.05)) +
  scale_y_continuous(breaks = breaks_pretty(n=6, min.n = 4), labels = fancy_scientific)+
  xlab("") +
  ylab("Particle number concentration [#/g]")+
  guides(shape=guide_legend(title="Day"))

ggsave("poster_particle_conc.png", height = 4, width = 9)

```

### Poster detection limit

```{r}
peak_width <- 6

#size/mass detection limit
poster_dl_sizemass <- poster_processed %>%
  filter(sample_group == "PMBLK") %>%
  unnest(peaks) %>%
  group_by(isotope) %>%
  summarise(h_thr = median(h_thr, na.rm = TRUE),
            mass_thr = max(mass_thr, na.rm = TRUE),
            peak_width = median(peak_width, na.rm = TRUE)) %>%
  mutate(
    mass_detection_limit = peak_width * mass_thr/2,
    size_detection_limit = case_when(
      isotope == "Au" ~ ((6*mass_detection_limit)/(pi*19.32*1*1000))^(1/3),
      isotope == "Ti" ~ ((6*mass_detection_limit)/(pi*4.17*0.59880240*1000))^(1/3),
      isotope == "Cr" ~ ((6*mass_detection_limit)/(pi*5.22*0.68399453*1000))^(1/3),
      isotope == "Cu" ~ ((6*mass_detection_limit)/(pi*6.31*0.79886864*1000))^(1/3)
  ))

```

Peak width of approx 6 appears to be representative for all particle types.

### Poster precision

Per sample, per isotope.

```{r}

poster_dls <- bind_rows(
  poster_processed %>%
    group_by(isotope) %>%
    summarise(
      particle_conc = mean(particle_conc_LOD),
      mass_conc = mean(mass_conc_LOD)
    ) %>% pivot_longer(cols = c(particle_conc, mass_conc), names_to = "parameter", values_to = "detection_limit"),
  poster_dl_sizemass %>% select(isotope, size_detection_limit) %>% rename(c(detection_limit = size_detection_limit)) %>% mutate(parameter = "mean_size")
)

# fit lm for each spike level, parameter (pnc/mass/size) ~ day.
# comment: Warning in code is due to sample without replicates.

poster_precision <-
  poster_processed %>%
  mutate(
    day = as.factor(day)
  ) %>%
  filter(str_detect(sample_group, "^\\w$")) %>%
  mutate(sample_group = case_when(
    str_detect(sample_group, "A|B|C") ~ "Farm",
    str_detect(sample_group, "G|H|I") ~ "Harbor",
    str_detect(sample_group, "J|K|L") ~ "Surveillance"
  )) %>%
  select(-n_particles, -transport_efficiency) %>%
  pivot_longer(
    cols = c(mean_size, mass_conc, particle_conc),
    names_to = "parameter",
    values_to = "value"
  ) %>%
  group_by(parameter, isotope, sample_group) %>%
  nest() %>%
  ungroup() %>%
  mutate(
    anovas = map(
      data,
      ~ .x %>%
        lm(formula = value ~ day) %>%
        anova() %>%
        tidy() %>%
        select(term, sumsq, meansq) %>%
        pivot_wider(names_from = term, values_from = c(meansq, sumsq))
    ),
    grand_means = map(data, ~ .x %>%
      ungroup() %>%
      summarise(grand_mean = mean(value))),
    rsd = map(data, ~ .x %>%
      ungroup() %>%
      summarise(rsd = sd(value) / mean(value))),
  ) %>%
  unnest(anovas, grand_means, rsd) %>%
  # Precision parameters
  mutate(
    RSE_repeatability = sqrt(meansq_Residuals) / grand_mean,
    RSE_intermediate_precision = sqrt(
      sqrt(meansq_Residuals)^2 +
        sqrt(
          (meansq_day - meansq_Residuals) / 3
          #
        )^2
    ) / grand_mean
  ) %>%
  left_join(poster_dls, by = join_by(isotope, parameter))


# size table
poster_precision %>% filter(parameter == "mean_size") %>% select(isotope, sample_group, isotope, grand_mean, RSE_repeatability,RSE_intermediate_precision, rsd, detection_limit) %>% 
  mutate(RSE_intermediate_precision = if_else(RSE_intermediate_precision %>% is.na(), rsd, RSE_intermediate_precision )) %>% 
  mutate(detection_limit = detection_limit/(10^(-9))) %>% 
  select(-rsd) %>% 
  select(isotope, sample_group, grand_mean, detection_limit, RSE_repeatability, RSE_intermediate_precision) %>% 
  gt(groupname_col = "isotope",
     rowname_col = "sample_group"
    ) %>% 
  cols_label(
    grand_mean = html("Mean [nm]"),
    RSE_repeatability = html("RSD<sub>Repeatability</sub>"),
    RSE_intermediate_precision = html("RSD<sub>Intermediate precision</sub>"),
    detection_limit = html("Detection limit [nm]")
  ) %>% tab_header("Diameter") %>% 
   tab_footnote(
    footnote = md("RSD is presented as variance was lower between days than within days."),
    locations = cells_body(
      columns = vars(RSE_intermediate_precision),
      rows = RSE_intermediate_precision <= RSE_repeatability),
    placement = "left") %>% 
  opt_footnote_marks(marks = "standard") %>% 
  fmt_number(decimals = 0) %>% 
  fmt_percent(columns = matches("rsd|RSE"), decimals = 0) %>% 
  gtsave(filename = "poster_preclod_size.docx")

# mass conc table
poster_precision %>% filter(parameter == "mass_conc") %>% select(isotope, sample_group, isotope, grand_mean, RSE_repeatability,RSE_intermediate_precision, rsd, detection_limit ) %>% 
  mutate(RSE_intermediate_precision = if_else(RSE_intermediate_precision %>% is.na(), rsd, RSE_intermediate_precision )) %>% 
  select(-rsd) %>% 
  select(isotope, sample_group, grand_mean, detection_limit, RSE_repeatability, RSE_intermediate_precision) %>% 
  gt(groupname_col = "isotope",
     rowname_col = "sample_group"
    ) %>% 
  cols_label(
    grand_mean = html("Mean [ng/g]"),
    RSE_repeatability = html("RSD<sub>Repeatability</sub>"),
    RSE_intermediate_precision = html("RSD<sub>Intermediate precision</sub>"),
    detection_limit = html("Detection limit [ng/g]")
  ) %>% tab_header("Mass concentration") %>% 
   tab_footnote(
    footnote = md("RSD is presented as variance was lower between days than within days."),
    locations = cells_body(
      columns = vars(RSE_intermediate_precision),
      rows = RSE_intermediate_precision <= RSE_repeatability),
    placement = "left") %>% 
  opt_footnote_marks(marks = "standard") %>% 
  fmt_number(decimals = 0) %>% 
    fmt_number(columns = matches("detection"), decimals = 1) %>% 
  fmt_percent(columns = matches("rsd|RSE"), decimals = 0) %>% 

  gtsave(filename = "poster_preclod_mass.docx")

  # part conc table
poster_precision %>% filter(parameter == "particle_conc") %>% select(isotope, sample_group, isotope, grand_mean, RSE_repeatability,RSE_intermediate_precision, rsd, detection_limit ) %>% 
  mutate(RSE_intermediate_precision = if_else(RSE_intermediate_precision %>% is.na(), rsd, RSE_intermediate_precision )) %>% 
  select(-rsd) %>% 
  select(isotope, sample_group, grand_mean, detection_limit, RSE_repeatability, RSE_intermediate_precision) %>% 
  gt(groupname_col = "isotope",
     rowname_col = "sample_group"
    ) %>% 
  cols_label(
    grand_mean = html("Mean [#/g]"),
    RSE_repeatability = html("RSD<sub>Repeatability</sub>"),
    RSE_intermediate_precision = html("RSD<sub>Intermediate precision</sub>"),
    detection_limit = html("Detection limit [#/g]")
  ) %>% tab_header("Particle concentration") %>% 
   tab_footnote(
    footnote = md("RSD is presented as variance was lower between days than within days."),
    locations = cells_body(
      columns = vars(RSE_intermediate_precision),
      rows = RSE_intermediate_precision <= RSE_repeatability),
    placement = "left") %>% 
  opt_footnote_marks(marks = "standard") %>% 
  fmt_number(decimals = 0) %>% 
  fmt_percent(columns = matches("rsd|RSE"), decimals = 0) %>% 
  fmt_scientific(
              columns = c(3,4),
              decimals = 1) %>% 
  gtsave(filename = "poster_preclod_pnc.docx")

```

```{r}
# poster_table %>% gt() %>%
#   fmt_number(
#     columns = 3:4,
#     decimals = 2
#   ) %>%
# fmt_scientific(
#               columns = 5,
#               decimals = 2) %>% 
#   gtsave(filename = "poster_table.docx")
```
## Contamination from homogenization

```{r}
contamination_prpoly_noinc <- sp_classifier("~/sp-data/201017_MultiMe_prelim_prbnopmex_2308poster_reanalysis.b/") %>%
  filter(isotope %in% c("Ti", "Cu", "Cr")) %>%
  mutate(rawdata = future_map(filepath, ~ sp_reader(.) %>%
    mutate(time = row_number() / 10000))) %>%
  unnest(rawdata) %>% 
  mutate(
    
        sample_name = case_when(
          sample_name == "PrB no pmex" ~ "Homogenized UPW",
          sample_name == "PrPoly no pmex" ~ "Homogenized PE",
          TRUE ~ "WRONG SAMPLE"
        )) %>% 
  mutate(
    sample_name = factor(sample_name, levels = c(
      "Homogenized UPW",
      "Homogenized PE"
    )))

cont_plot <- contamination_prpoly_noinc %>%
  # filter(isotope == "Cu", sample_name == "PrB no pmex") %>%
  ggplot(aes(x = time, y = counts)) +
  geom_line() +
  facet_grid(sample_name ~ isotope)+
  # scale_y_continuous(breaks = breaks_pretty(n=6, min.n = 4), labels = fancy_scientific)+
  xlab("Counts [#/100\u03BCs]") +
  ylab("Time [s]")



ggsave(cont_plot, filename = "cont_plot.tiff", width = 10, height = 5.25 )

```

## Artifact

```{r}
tar_read(au_sizes) %>%
  filter(
    sample_name %>% str_detect("Au RM"),
    !str_detect(sample_name, "ERROR|200")
  ) %>%
  unnest(peaks) %>%
  mutate(
    sample_name = factor(sample_name, levels = c(
      "Au RM 10.9",
      "Au RM 20.3",
      "Au RM 30",
      "Au RM 39",
      "Au RM 51",
      "Au RM 63",
      "Au RM 83",
      "Au RM 100"

    ))
  ) %>%
  group_by(sample_name) %>%
  mutate(particle_size_ref = str_extract(sample_name, "[\\d\\.]+$") %>% as.numeric()) %>%
  ggplot(aes(particle_size)) +
  geom_vline(aes(xintercept = particle_size_ref), color = "black", linetype = "longdash") +
  geom_histogram(position = "identity", binwidth = 0.5, alpha = 0.6) +
  facet_wrap(~sample_name, scale = "free", ncol = 2) +

  xlim(c(0, 120)) +
  theme_linedraw(base_size = 15) +
  theme(panel.grid.major = element_line(size = 0.05)) +
  theme(panel.grid.minor = element_line(size = 0.05)) +
  scale_fill_manual(values = c("#436EEE", "#8B3626")) +
  xlab("Particle diameter [nm]") +
  ylab("Frequency [#]")

ggsave(filename = "artifat_au_sizes.tiff", width = 15, height = 18)

```

